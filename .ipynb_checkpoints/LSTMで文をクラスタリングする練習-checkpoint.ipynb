{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chainerのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo chmod 777 /usr/local/lib/python3.6/dist-packages/__pycache__/\n",
    "# !pip3 install chainer\n",
    "# !git clone https://github.com/chainer/chainer.git\n",
    "# !export PYTHONPATH=chainer/examples/text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "# 活性化関数等を管理する関数\n",
    "import chainer.functions as F\n",
    "# BiasとWeightを管理する関数\n",
    "import chainer.links as L\n",
    "import numpy\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "import sys\n",
    "sys.path.append('/home/vagrant/chainer/examples/text_classification')\n",
    "import nets\n",
    "import nlp_utils\n",
    "\n",
    "# 継承クラスの作成、class名はEncoder,別ファイルのchainer.Chainを継承する\n",
    "class Encoder(chainer.Chain):\n",
    "    \n",
    "    # def __init__はコンストラクタ\n",
    "    # 中には、どのように生成するか、どのようなデータを持たせるかなど、といった情報を定義する\n",
    "    def __init__(self, w):\n",
    "        # super(Encoder, self).__init__()で別ファイルのスーパークラス（chainer.Chain）のメソッドを呼び出すことが出来る。\n",
    "        super(Encoder, self).__init__()\n",
    "        # 300はWord2Vecの次元数\n",
    "        self.out_units = 300\n",
    "        \n",
    "        # with構文でファイルを扱う\n",
    "        # Chainクラスで重みの更新がされるのは self.init_scope()内に書いている linkオブジェクト\n",
    "        with self.init_scope():\n",
    "            self.embed = lambda x: F.embed_id(x, w)\n",
    "            # 学習するLSTMの形を設定する\n",
    "            self.encoder = L.NStepLSTM(n_layers=1, in_size=300, out_size=self.out_units, dropout=0.5)\n",
    "    \n",
    "    # 単語のID列をWord2Vecのベクトル列のデータに変換し、LSTMにわたす\n",
    "    def forward(self, xs):\n",
    "        exs = nets.sequence_embed(self.embed, xs)\n",
    "        last_h, last_c, ys = self.encoder(None, None, exs)\n",
    "        return last_h[-1]\n",
    "    \n",
    "# trainでモデルの学習を行う\n",
    "def train(labels, features, w):\n",
    "    # set型、集合型に変換する\n",
    "    n_class = len(set(labels))\n",
    "    print(f'# data: {len(features)}')\n",
    "    print(f'# class: {n_class}')\n",
    "    \n",
    "    # 学習用データをchainerのiteratorの形にしておく\n",
    "    pairs = [(vec, numpy.array([cls], numpy.int32)) for vec, cls in zip(features, labels)]\n",
    "    train_iter = chainer.iterators.SerialIterator(pairs, batch_size=16)\n",
    "    \n",
    "    # 学習するモデルをちゃいねｒのサンプルプログラムのTextClassifierクラスを用いて設定する\n",
    "    # 二値分類であるためカテゴリ数には2を指定、モデルはEncoderクラスのLSTMを指定する\n",
    "    model = nets.TextClassifier(Encoder(w), n_class)\n",
    "    \n",
    "    # 最適化にはAdamを選択\n",
    "    # ニューラルネットの学習方法を指定します。SGDは最も単純なものです。\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    # 学習させたいパラメータを持ったChainをオプティマイザーにセットします。\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "    \n",
    "    # optimizerを使用してupdatersでパラメータを更新する\n",
    "    updater = training.updaters.StandardUpdater(train_iter, optimizer, converter=convert_seq)\n",
    "    # Trainerを用意する。updaterを渡すことで使える。epochを指定する。outはデータを保存する場所。\n",
    "    trainer = training.Trainer(updater, (8, 'epoch'), out='./result/dl')\n",
    "    \n",
    "    # 下記で学習経過を確認する\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'elapsed_time']))\n",
    "    \n",
    "    # 学習スタート\n",
    "    trainer.run()\n",
    "    return model\n",
    "\n",
    "# 分類の実行\n",
    "def classify(features, model):\n",
    "    with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "        # chainerのpredict関数からは各カテゴリに対数確率値が帰ってくる\n",
    "        prob = model.predict(features, softmax=True)\n",
    "    answers = model.xp.argmax(prob, axis=1)\n",
    "    return answers\n",
    "\n",
    "# Word2Vecを元に作成したボキャブラリを用いて、単語を単語のIDに変換する\n",
    "def convert_into_features_using_vocab(sentences, vocab):\n",
    "    contents = []\n",
    "    for doc_id, sent, tokens in sentences:\n",
    "        features = [token['lemma'] for token in tokens]\n",
    "        contents.append(features)\n",
    "    features = transform_to_array(contents, vocab, with_label=False)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_utilsが読み込めないため持ってくる\n",
    "import collections\n",
    "import io\n",
    "\n",
    "import numpy\n",
    "\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "\n",
    "\n",
    "def split_text(text, char_based=False):\n",
    "    if char_based:\n",
    "        return list(text)\n",
    "    else:\n",
    "        return text.split()\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def make_vocab(dataset, max_vocab_size=20000, min_freq=2):\n",
    "    counts = collections.defaultdict(int)\n",
    "    for tokens, _ in dataset:\n",
    "        for token in tokens:\n",
    "            counts[token] += 1\n",
    "\n",
    "    vocab = {'<eos>': 0, '<unk>': 1}\n",
    "    for w, c in sorted(counts.items(), key=lambda x: (-x[1], x[0])):\n",
    "        if len(vocab) >= max_vocab_size or c < min_freq:\n",
    "            break\n",
    "        vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def read_vocab_list(path, max_vocab_size=20000):\n",
    "    vocab = {'<eos>': 0, '<unk>': 1}\n",
    "    with io.open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        for l in f:\n",
    "            w = l.strip()\n",
    "            if w not in vocab and w:\n",
    "                vocab[w] = len(vocab)\n",
    "            if len(vocab) >= max_vocab_size:\n",
    "                break\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def make_array(tokens, vocab, add_eos=True):\n",
    "    unk_id = vocab['<unk>']\n",
    "    eos_id = vocab['<eos>']\n",
    "    ids = [vocab.get(token, unk_id) for token in tokens]\n",
    "    if add_eos:\n",
    "        ids.append(eos_id)\n",
    "    return numpy.array(ids, numpy.int32)\n",
    "\n",
    "\n",
    "def transform_to_array(dataset, vocab, with_label=True):\n",
    "    if with_label:\n",
    "        return [(make_array(tokens, vocab), numpy.array([cls], numpy.int32))\n",
    "                for tokens, cls in dataset]\n",
    "    else:\n",
    "        return [make_array(tokens, vocab)\n",
    "                for tokens in dataset]\n",
    "\n",
    "\n",
    "def convert_seq(batch, device=None, with_label=True):\n",
    "    def to_device_batch(batch):\n",
    "        if device is None:\n",
    "            return batch\n",
    "        elif device < 0:\n",
    "            return [chainer.dataset.to_device(device, x) for x in batch]\n",
    "        else:\n",
    "            xp = cuda.cupy.get_array_module(*batch)\n",
    "            concat = xp.concatenate(batch, axis=0)\n",
    "            sections = numpy.cumsum([len(x)\n",
    "                                     for x in batch[:-1]], dtype=numpy.int32)\n",
    "            concat_dev = chainer.dataset.to_device(device, concat)\n",
    "            batch_dev = cuda.cupy.split(concat_dev, sections)\n",
    "            return batch_dev\n",
    "\n",
    "    if with_label:\n",
    "        return {'xs': to_device_batch([x for x, _ in batch]),\n",
    "                'ys': to_device_batch([y for _, y in batch])}\n",
    "    else:\n",
    "        return to_device_batch([x for x in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# data: 80\n",
      "# class: 2\n",
      "epoch       main/loss   main/accuracy  elapsed_time\n",
      "\u001b[J1           0.708339    0.5375         2.17802       \n",
      "\u001b[J2           0.544473    0.75           4.38155       \n",
      "\u001b[J3           0.471009    0.7875         6.59997       \n",
      "\u001b[J4           0.325815    0.8875         8.89658       \n",
      "\u001b[J5           0.182776    0.95           11.1065       \n",
      "\u001b[J6           0.104519    0.975          13.2824       \n",
      "\u001b[J7           0.0624946   1              15.367        \n",
      "\u001b[J8           0.0207733   1              17.4714       \n",
      "correct    1 1 農林産品としてはこのほか、カシューナッツや木材の輸出もある\n",
      "correct    0 0 ヨーロッパ最大の草原のひとつ （ホルトバージ国立公園）\n",
      "correct    0 0 なお紅茶ではなく中国茶が多く飲まれている[32]\n",
      "correct    0 0 フィリピン海、南シナ海、セレベス海に囲まれる\n",
      "incorrect   1 0 オイルパーム（アブラヤシ）から精製されるパームオイル（パーム油、ヤシ油）は、植物油の原料の一つで、1990年代後半日本国内では菜種油・大豆油に次いで第3位で、食用・洗剤・シャンプー・化粧品の原料として需要の増大が見込まれている\n",
      "correct    0 0 この砂漠は風によって絶えず景色が変化する砂砂漠（エルグ）、ごつごつした石や砂利と乾燥に強い植物がわずかにみられる礫砂漠、または植生のない岩肌がむき出しになっている岩石砂漠が広がっており、砂砂漠は一部であり、大部分は礫砂漠と岩石砂漠で占められている[10]\n",
      "incorrect   0 1 国土を囲む海域には北極海の一部であるバレンツ海、白海、カラ海、ラプテフ海、東シベリア海と、太平洋の一部であるベーリング海、オホーツク海、日本海、そして西のバルト海と西南の黒海があり、海岸線は37,000kmに及ぶ\n",
      "incorrect   1 0 羊肉はウズベキスタン国内でヒツジの放牧が盛んであることから一般的に販売されている肉であり、様々なウズベキスタン料理に使用されている\n",
      "correct    0 0 大鑽井盆地より更に西はグレートサンディ砂漠、グレートビクトリア砂漠、ギブソン砂漠等の砂漠が広がり、人はあまり住んでいない\n",
      "correct    1 1 資源には見るべきものがなく、わずかに塩が生産されるのみである\n",
      "correct    1 1 塩を4万トン生産する\n",
      "incorrect   0 1 ガンディーは「塩の行進」を開始したが成功しなかった\n",
      "correct    1 1 鉱業の対象となる唯一の資源は塩である\n",
      "correct    0 0 東はアドリア海、西でティレニア海とリグリア海、南でイオニア海と地中海に面している\n",
      "correct    1 1 有機鉱物以外では、世界第8位となるカリ塩 （KCl） 、同10位となる塩 （NaCl） がある\n",
      "incorrect   1 0 森林 （36%） も多い\n",
      "correct    1 1 また茶を飲む習慣も広まったが、茶は砂糖とミントを入れて沸騰させ濃いめにし小さなグラスで3回に分けて飲むというスタイルである[32]\n",
      "incorrect   1 0 国土の多くは草原となっており馬や牛や羊が飼育されている\n",
      "correct    0 0 西にアンデス山脈、東にはパンパと呼ばれる大草原が広がる\n",
      "correct    0 0 ウクライナの国土のほとんどは、肥沃な平原、ステップ（草原）、高原で占められている\n",
      "incorrect   1 0 基本的には豆やトウモロコシ、鳥肉を原材料に使ったメニューが主体になっており、他にも米や魚類、牛肉なども使われることが多く、一見単純に見えて繊細な味がその人気の理由とされている\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "sys.path.append('src')\n",
    "import sqlitedatastore as datastore\n",
    "from annoutil import find_xs_in_y\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def extract_w_and_vocab(model):\n",
    "    w_ls = []\n",
    "    vocab = {}\n",
    "    for word in model.wv.index2word:\n",
    "        vocab[word] = len(vocab)\n",
    "        w_ls.append(model[word])\n",
    "    for word in ['<eos>', '<unk>']:\n",
    "        vocab[word] = len(vocab)\n",
    "        w_ls.append(2 * numpy.random.rand(300) - 1)\n",
    "        \n",
    "    return numpy.array(w_ls).astype(numpy.float32), vocab\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datastore.connect()\n",
    "    # Word2Vecをgensimライブラリを用いて読み込み\n",
    "    w2v_model = gensim.models.Word2Vec.load('./data/ja/ja.bin')\n",
    "    # chainerで扱いやすいようにwとvocabに分割\n",
    "    w, vocab = extract_w_and_vocab(w2v_model)\n",
    "    # ラベル付きデータ読み込み\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    df = pd.read_csv('data/svmdata100.csv', header=0, index_col=0, )\n",
    "    # dfをzipを使い複数列、1行ずつ取り出す\n",
    "    for label, doc_id, sent_id in zip(df['#label'].astype('int64'), df['doc_id'], df['sentence_id'].astype('int64')):\n",
    "        sent = datastore.get_annotation(doc_id, 'sentence')[sent_id]\n",
    "        tokens = find_xs_in_y(datastore.get_annotation(doc_id, 'token'), sent)\n",
    "        sentences.append((doc_id, sent, tokens))\n",
    "        labels.append(label)\n",
    "        \n",
    "    # 8割を学習に使用\n",
    "    num_train = int(len(sentences) * 0.8)\n",
    "    sentences_train = sentences[:num_train]\n",
    "    labels_train = labels[:num_train]\n",
    "    \n",
    "    # ID変換\n",
    "    features = convert_into_features_using_vocab(sentences_train, vocab)\n",
    "    \n",
    "    # 学習開始\n",
    "    model = train(labels_train, features, w)\n",
    "    \n",
    "    # 学習モデルをファイルに保存\n",
    "    chainer.serializers.save_npz('result/model_dl.npz', model)\n",
    "    numpy.save('result/w_dl.npy', w)\n",
    "    \n",
    "    with open('result/vocab_dl.json', 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    # 分類の実行\n",
    "    # テスト用データの作成\n",
    "    features_test = convert_into_features_using_vocab(sentences[num_train:], vocab)\n",
    "    # 実行\n",
    "    predicteds = classify(features_test, model)\n",
    "    \n",
    "    for predicted, (doc_id, sent, tokens), label in zip(predicteds, sentences[num_train:], labels[num_train:]):\n",
    "        # 結果の確認\n",
    "        text = datastore.get(doc_id, ['content'])['content']\n",
    "        \n",
    "        if predicted == label:\n",
    "            print('correct ', ' ', label, predicted, text[sent['begin']:sent['end']])\n",
    "        else:\n",
    "            print('incorrect', ' ', label, predicted, text[sent['begin']:sent['end']])\n",
    "    \n",
    "    datastore.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
