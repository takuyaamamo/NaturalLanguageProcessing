{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chainerのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo chmod 777 /usr/local/lib/python3.6/dist-packages/__pycache__/\n",
    "# !pip3 install chainer\n",
    "# !git clone https://github.com/chainer/chainer.git\n",
    "# !export PYTHONPATH=chainer/examples/text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'convert_seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e60e11305108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnlp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_to_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 継承クラスの作成、class名はEncoder,別ファイルのchainer.Chainを継承する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'convert_seq'"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "# 活性化関数等を管理する関数\n",
    "import chainer.function as F\n",
    "# BiasとWeightを管理する関数\n",
    "import chainer.links as L\n",
    "import numpy\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "import sys\n",
    "sys.path.append('/home/vagrant/chainer/examples/text_classification/')\n",
    "import nets\n",
    "import nlp_utils\n",
    "\n",
    "# 継承クラスの作成、class名はEncoder,別ファイルのchainer.Chainを継承する\n",
    "class Encoder(chainer.Chain):\n",
    "    \n",
    "    # def __init__はコンストラクタ\n",
    "    # 中には、どのように生成するか、どのようなデータを持たせるかなど、といった情報を定義する\n",
    "    def __init__(self, w):\n",
    "        # super(Encoder, self).__init__()で別ファイルのスーパークラス（chainer.Chain）のメソッドを呼び出すことが出来る。\n",
    "        super(Encoder, self).__init__()\n",
    "        # 300はWord2Vecの次元数\n",
    "        self.out_units = 300\n",
    "        \n",
    "        # with構文でファイルを扱う\n",
    "        # Chainクラスで重みの更新がされるのは self.init_scope()内に書いている linkオブジェクト\n",
    "        with self.init_scope():\n",
    "            self.embed = lambda x: F.embed_id(x, w)\n",
    "            # 学習するLSTMの形を設定する\n",
    "            self.encoder = L.NStepLSTM(n_layers=1, in_size=300, out_size=self.out_units, dropout=0.5)\n",
    "    \n",
    "    # 単語のID列をWord2Vecのベクトル列のデータに変換し、LSTMにわたす\n",
    "    def forward(self, ws):\n",
    "        exs = nets.sequence_embed(self.embed, xs)\n",
    "        last_h, last_c, ys = self.encoder(None, None, exs)\n",
    "        return last_h[-1]\n",
    "    \n",
    "# trainでモデルの学習を行う\n",
    "def train(labels, features, w):\n",
    "    # set型、集合型に変換する\n",
    "    n_class = len(set(labels))\n",
    "    print(f'# data: {len(features)}')\n",
    "    print(f'# class: {n_class}')\n",
    "    \n",
    "    # 学習用データをchainerのiteratorの形にしておく\n",
    "    pairs = [(vec, numpy.array([cls], numpy.int32)) for vec, cls in zip(features, labels)]\n",
    "    train_iter = chainer.iterators.Seriallterator(pairs, batch_size=16)\n",
    "    \n",
    "    # 学習するモデルをちゃいねｒのサンプルプログラムのTextClassifierクラスを用いて設定する\n",
    "    # 二値分類であるためカテゴリ数には2を指定、モデルはEncoderクラスのLSTMを指定する\n",
    "    model = nets.TextClassifier(Encoder(w), n_class)\n",
    "    \n",
    "    # 最適化にはAdamを選択\n",
    "    # ニューラルネットの学習方法を指定します。SGDは最も単純なものです。\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    # 学習させたいパラメータを持ったChainをオプティマイザーにセットします。\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "    \n",
    "    # optimizerを使用してupdatersでパラメータを更新する\n",
    "    updater = training.updaters.StandardUpdater(train_iter, optimizer, converter=convert_seq)\n",
    "    # Trainerを用意する。updaterを渡すことで使える。epochを指定する。outはデータを保存する場所。\n",
    "    trainer = training.Trainer(updater, (8, 'epoch'), out='./result/dl')\n",
    "    \n",
    "    # 下記で学習経過を確認する\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'elapsed_time']))\n",
    "    \n",
    "    # 学習スタート\n",
    "    trainer.run()\n",
    "    return model\n",
    "\n",
    "# 分類の実行\n",
    "def classify(features, model):\n",
    "    with chainer.using_config('train', False), chainer.no_backprop_model():\n",
    "        # chainerのpredict関数からは各カテゴリに対数確率値が帰ってくる\n",
    "        prob = model.predict(features, softmax=True)\n",
    "    answers = model.xp.argmax(prob, axis=1)\n",
    "    return answers\n",
    "\n",
    "# Word2Vecを元に作成したボキャブラリを用いて、単語を単語のIDに変換する\n",
    "def convert_into_features_using_vocab(sentences, vocab):\n",
    "    contents = []\n",
    "    for doc_id, sent, tokens in sentences:\n",
    "        features = [token['lemma'] for token in tokens]\n",
    "        contents.append(features)\n",
    "    features = transform_to_array(contents, vocab, with_label=False)\n",
    "    return feautures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vagrant/.local/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transform_to_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ec38768bfb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# ID変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_into_features_using_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# 学習開始\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0ef0084684e0>\u001b[0m in \u001b[0;36mconvert_into_features_using_vocab\u001b[0;34m(sentences, vocab)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeautures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transform_to_array' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "sys.path.append('src')\n",
    "import sqlitedatastore as datastore\n",
    "from annoutil import find_xs_in_y\n",
    "import pandas as pd\n",
    "\n",
    "def extract_w_and_vocab(model):\n",
    "    w_ls = []\n",
    "    vocab = {}\n",
    "    for word in model.wv.index2word:\n",
    "        vocab[word] = len(vocab)\n",
    "        w_ls.append(model[word])\n",
    "    for word in ['<eos>', '<unk>']:\n",
    "        vocab[word] = len(vocab)\n",
    "        w_ls.append(2 * numpy.random.rand(300) - 1)\n",
    "        \n",
    "    return numpy.array(w_ls).astype(numpy.float32), vocab\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datastore.connect()\n",
    "    # Word2Vecをgensimライブラリを用いて読み込み\n",
    "    w2v_model = gensim.models.Word2Vec.load('./data/ja/ja.bin')\n",
    "    # chainerで扱いやすいようにwとvocabに分割\n",
    "    w, vocab = extract_w_and_vocab(w2v_model)\n",
    "    # ラベル付きデータ読み込み\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    df = pd.read_csv('data/svmdata100.csv', header=0, index_col=0, )\n",
    "    # dfをzipを使い複数列、1行ずつ取り出す\n",
    "    for label, doc_id, sent_id in zip(df['#label'].astype('int64'), df['doc_id'], df['sentence_id'].astype('int64')):\n",
    "        sent = datastore.get_annotation(doc_id, 'sentence')[sent_id]\n",
    "        tokens = find_xs_in_y(datastore.get_annotation(doc_id, 'token'), sent)\n",
    "        sentences.append((doc_id, sent, tokens))\n",
    "        labels.append(label)\n",
    "        \n",
    "    # 8割を学習に使用\n",
    "    num_train = int(len(sentences) * 0.8)\n",
    "    sentences_train = sentences[:num_train]\n",
    "    labels_train = labels[:num_train]\n",
    "    \n",
    "    # ID変換\n",
    "    features = convert_into_features_using_vocab(sentences_train, vocab)\n",
    "    \n",
    "    # 学習開始\n",
    "    model = train(labels_train, features, w)\n",
    "    \n",
    "    # 学習モデルをファイルに保存\n",
    "    chainer.serializers.save_npz('result/model_dl.npz', model)\n",
    "    numpy.save('result/w_dl.npy', w)\n",
    "    \n",
    "    with open('result/vocab_dl.json', 'w') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    # 分類の実行\n",
    "    # テスト用データの作成\n",
    "    features_test = convert_into_features_using_vocab(sentences[num_train:], vocab)\n",
    "    # 実行\n",
    "    predicteds = classify(features_test, model)\n",
    "    \n",
    "    for predicted, (doc_id, sent, tokens), label in zip(predicteds, sentences[num_train:], labels[num_train:]):\n",
    "        # 結果の確認\n",
    "        text = datastore.get(doc_id, ['content'])['content']\n",
    "        \n",
    "        if predected == lavel:\n",
    "            print('correct ', ' ', label, predicted, text[sent['begin']:sent['end']])\n",
    "        else:\n",
    "            print('incorrect', ' ', label, predicted, text[sent['begin']:sent['end']])\n",
    "    \n",
    "    datastore.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
