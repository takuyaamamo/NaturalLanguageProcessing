{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデルのN-gramとは\n",
    "\n",
    "- 言語モデル  \n",
    "単語の並びに対して、その発生確率を返すもの。通常はたんｇの出現頻度にもとづいて言語モデルが算出される。\n",
    "- N-gramモデル  \n",
    "言語モデルの一種で「単語の発生確率がその前のN-1個の単語にのみ依存する」と仮定したモデル。  \n",
    "（機械学習等に使うテキストの特徴量のことをN-gramという事もある）\n",
    "\n",
    "例）  \n",
    "N=3のときの、3-gramモデルは、単語の出現確率が、その前の2（=3-1）個の単語にのみ依存すると仮定したモデル。  \n",
    "- 3-gramを求める  \n",
    "$$\n",
    "P(\"[BOS]\"　\"古く\"　\"から\"　\"人\"　\"が\"　\"居住\"　\"する\"　\"[EOS]\") = \\\\\n",
    "P(\"[BOS]\",\"古く\") \\times P(\"から\" | \"[BOS]\",\"古く\") \\times P(\"人\" | \"古く\",\"から\") \\times P(\"が\" | \"から\",\"人\") \\times P(\"居住\" |  \"人\",\"が\") \\times P(\"する\" | \"が\",\"居住\") \\times P(\"[EOS]\" | \"居住\",\"する\")\n",
    "$$\n",
    "\n",
    "単語「人」が出現する確率に注目すると\n",
    "\n",
    "$$\n",
    "P(\"人\" | \"古く\",\"から\")= \\\\\n",
    "\\displaystyle \\frac{ コーパスの中で単語の「古く」「から」が、この順で現れたあとに「人」が現れる回数 }{ コーパスの中で単語の「古く」「から」が、この順で現れる回数 } \n",
    "$$\n",
    "\n",
    "として計算する事ができます。  \n",
    "同様に全ての単語に対して3-gramを計算しておくと\n",
    "$$\n",
    "P(\"[BOS]\" \"古く\" \"から\" \"人\" \"が\"　\"居住\"　\"する\"　\"[EOS]\") \\\\\n",
    "P(\"[BOS]\" \"古く\" \"が\" \"人\" \"から\"　\"居住\"　\"する\"　\"[EOS]\")\n",
    "$$\n",
    "のどちらが自然な文かを比較できる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramを計算してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltkライブラリをインストールする  \n",
    "Pythonで自然言語処理を行うために便利な機能を持つライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "Collecting six (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, nltk\n",
      "Successfully installed nltk-3.4.5 six-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqlite3接続用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = None\n",
    "\n",
    "# データベース接続\n",
    "def connect():\n",
    "    # global変数でconnを呼び出し\n",
    "    global conn\n",
    "    # データベースの場所を指定\n",
    "    conn = sqlite3.connect('/vagrant/NaturalLanguageProcessing/data/sqlite3/sqlite3')\n",
    "\n",
    "# データベース接続終了\n",
    "def close():\n",
    "    # 終了\n",
    "    conn.close()\n",
    "\n",
    "# テーブル作成\n",
    "def create_table():\n",
    "    # executeでSQL構文作成、docsがあれば削除\n",
    "    conn.execute('DROP TABLE IF EXISTS docs')\n",
    "    # docsテーブルを新規作成\n",
    "    conn.execute('''CREATE TABLE docs (\n",
    "            id          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            content     TEXT,\n",
    "            meta_info   BLOB,\n",
    "            sentence    BLOB,\n",
    "            chunk       BLOB,\n",
    "            token       BLOB\n",
    "        )''')\n",
    "\n",
    "# データをインサートする\n",
    "def load(values):\n",
    "    # exucutemany()はvaluesに指定したパラメータ順序またはマッピングを?に入れて実行できる\n",
    "    conn.executemany('INSERT INTO docs (content, meta_info) VALUES (?,?)', values)\n",
    "    # 確定\n",
    "    conn.commit()\n",
    "\n",
    "# 一部のデータを見る\n",
    "def get(doc_id, fl):\n",
    "    #.fetchone()で指定した1行を取得\n",
    "    row_ls = conn.execute(f\"SELECT {','.join(fl)} FROM docs WHERE id = {doc_id}\").fetchone()\n",
    "    # row_ls = conn.execute('SELECT {} FROM docs WHERE id = ?'.format(','.join(fl)),(doc_id,)).fetchone()\n",
    "    row_dict = {}\n",
    "    # flとrow_lsで抜き出したデータをzipする\n",
    "    for key, value in zip(fl, row_ls):\n",
    "        row_dict[key] = value\n",
    "    return row_dict\n",
    "\n",
    "# id番号を抜き出す\n",
    "def get_all_ids(limit, offset=0):\n",
    "    return [record[0] for record in\n",
    "            # limitで取得上限、OFFSETで開始位置を指定してデータを抜き出す。そのデータの1番目id番号を抜き出す\n",
    "            conn.execute(f'SELECT id FROM docs LIMIT {limit} OFFSET {offset}')]\n",
    "            # conn.execute('SELECT id FROM docs LIMIT ? OFFSET ?',(limit, offset))]\n",
    "\n",
    "def set_annotation(doc_id, name, value):\n",
    "    conn.execute(f'UPDATE docs SET {name} = {json.dumps(value)} where id = {doc_id}')\n",
    "    conn.commit()\n",
    "\n",
    "# アノテーションを取得\n",
    "def get_annotation(doc_id, name):\n",
    "    # docsのid行をwhere idで指定しnameから取り出す\n",
    "    row = conn.execute(f'SELECT {name} FROM docs WHERE id = {doc_id}').fetchone()\n",
    "    if row[0] is not None:\n",
    "        return json.loads(row[0])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nリスト内包表記\\nfor x in xs:\\n    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\\n        return x\\n\\nxs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文内の単語のを取得するための関数を作成\n",
    "def find_xs_in_y(xs, y):\n",
    "    return [x for x in xs if y['begin'] <= x['begin'] and x['end'] <= y['end']]\n",
    "'''\n",
    "リスト内包表記\n",
    "for x in xs:\n",
    "    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\n",
    "        return x\n",
    "\n",
    "xs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#言語モデルの作成\n",
    "def create_laguage_model(doc_ids, N=3):\n",
    "    sents = []\n",
    "    \n",
    "    # コーパスとして文ごとに単語の原型のリストをsents変数に格納する\n",
    "    for doc_id in doc_ids:\n",
    "        all_tokens = get_annotation(doc_id, 'token')\n",
    "        \n",
    "        for sent in datastore.get_annotation(doc_id, 'sentence'):\n",
    "            tokens = find_xs_in_y(all_tokens, sent)\n",
    "            sents.append(['__BOS__'] + [token['lemma']\n",
    "                                        for token in tokens] + ['__EOS__'])\n",
    "    vocab = Vocabulary([word for sent in sents for word in sent])\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sents]\n",
    "    lm = MLE(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
