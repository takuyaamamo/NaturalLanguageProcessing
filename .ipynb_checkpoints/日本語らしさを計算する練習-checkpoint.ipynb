{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 日本語らしさを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文の中の単語をランダムに並び替えて、言語モデルを使って「どの程度日本語らしいか」を計算してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデル関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "# MLE(Maximum Likelihood Estimator):言語モデルを作成するライブラリ\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def find_xs_in_y(xs, y):\n",
    "    return [x for x in xs if y['begin'] <= x['begin'] and x['end'] <= y['end']]\n",
    "\n",
    "#言語モデルの作成\n",
    "def create_language_model(doc_ids, N=3):\n",
    "    sents = []\n",
    "    \n",
    "    # コーパスとして文ごとに単語の原型のリストをsents変数に格納する\n",
    "    for doc_id in doc_ids:\n",
    "        all_tokens = datastore.get_annotation(doc_id, 'token')\n",
    "        \n",
    "        for sent in datastore.get_annotation(doc_id, 'sentence'):\n",
    "            tokens = find_xs_in_y(all_tokens, sent)\n",
    "            # コーパスとして文ごとに単語の原型リストをsents変数に格納する。文頭と文末も単語として追加する。\n",
    "            sents.append(['__BOS__'] + [token['lemma'] for token in tokens] + ['__EOS__'])\n",
    "            \n",
    "    # 作成したコーパスにおける全単語の一覧をvocabに作成\n",
    "    vocab = Vocabulary([word for sent in sents for word in sent])\n",
    "    # ngramsを使用し文ごとに3つの単語の並びを取り出す\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sents]\n",
    "    '''\n",
    "    ngramの第一引数には単語リスト、第2引数には関数で指定したデフォルトの3を入れる\n",
    "    '''\n",
    "    lm = MLE(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語らしさの確率を計算する関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (算出した言語モデル, 単語列, n-gramのn)\n",
    "def calc_prob(lm, lemmas, N=3):\n",
    "    probability = 1.0\n",
    "    \n",
    "    # ngramsを使用しN=3の単語の並びを作成し、一つずつ言語モデルを計算する。\n",
    "    for ngram in ngrams(lemmas, N):\n",
    "        prob = lm.score(lm.vocab.lookup(ngram[-1]), lm.vocab.lookup(ngram[:-1]))\n",
    "        # 学習したコーパスにない単語に関しては並びの確率に10の-8乗を割り当てる。\n",
    "        prob = max(prob, le-8)\n",
    "        #  計算した単語の並びの確率をかけ合わせて、文の確率を計算する。\n",
    "        probability *= prob\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算開始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/vagrant/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/home/vagrant/.local/lib/python3.6/site-packages/IPython/extensions', '/home/vagrant/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# https://www.sejuku.net/blog/66459\n",
    "# システムに関する処理をまとめたライブラリのsysを読み込む\n",
    "import sys\n",
    "# 下記でライブラリを読み込めるパス一覧を表示できる。ここにパスを書き込むと異なる階層からライブラリを読み込む事が可能となる。\n",
    "print(sys.path)\n",
    "# sys.path.append(\"相対パス\")でsys.pathに追加、ここではディレクトリまでを指定する\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import sqlitedatastore as datastore\n",
    "import cabochaparser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_xs_in_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-527c8327bfb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 全てのidで言語モデルを作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_language_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# テストする文を設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3a3d0f8178bf>\u001b[0m in \u001b[0;36mcreate_language_model\u001b[0;34m(doc_ids, N)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_xs_in_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m# コーパスとして文ごとに単語の原型リストをsents変数に格納する。文頭と文末も単語として追加する。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__BOS__'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'__EOS__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_xs_in_y' is not defined"
     ]
    }
   ],
   "source": [
    "datastore.connect()\n",
    "\n",
    "# 全てのidで言語モデルを作成\n",
    "lm = create_language_model(datastore.get_all_ids(limit=-1), N=3)\n",
    "\n",
    "# テストする文を設定\n",
    "text = '古くから人が居住する。'\n",
    "\n",
    "# cabochaを実行し、textをtokenに分割する\n",
    "sentences, chunks, tokens = parser.parse(text)\n",
    "\n",
    "probabilities = set([])\n",
    "for i in range(1000):\n",
    "    # 2語目以降のtokenをシャッフルしtokens_に収納\n",
    "    tokens_ = tokens[1:]\n",
    "    random.shuffle(tokens_)\n",
    "    # 1語目とくっつける\n",
    "    tokens_shuffled = [tokens[0]] + tokens_\n",
    "    lemmas = ['__BOS__'] + [token['lemma'] for token in tokens_shuffled]\n",
    "    print(lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
