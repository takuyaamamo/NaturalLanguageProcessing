{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDFを計算する\n",
    "文書中に含まれる単語の重要度を評価する手法\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{tf}-\\mathrm{idf}(w, d) &=& \\mathrm{tf}(w, d) \\times \\mathrm{idf}(w)\\\\\n",
    "                               &=& 単語wの文書d中での出現回数 \\times \\log \\displaystyle \\frac{全文書数}{単語wが出現する文書数} \n",
    "                               \\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQlite用関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = None\n",
    "\n",
    "# データベース接続\n",
    "def connect():\n",
    "    # global変数でconnを呼び出し\n",
    "    global conn\n",
    "    # データベースの場所を指定\n",
    "    conn = sqlite3.connect('/vagrant/NaturalLanguageProcessing/data/sqlite3/sqlite3')\n",
    "\n",
    "# データベース接続終了\n",
    "def close():\n",
    "    # 終了\n",
    "    conn.close()\n",
    "\n",
    "# テーブル作成\n",
    "def create_table():\n",
    "    # executeでSQL構文作成、docsがあれば削除\n",
    "    conn.execute('DROP TABLE IF EXISTS docs')\n",
    "    # docsテーブルを新規作成\n",
    "    conn.execute('''CREATE TABLE docs (\n",
    "            id          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            content     TEXT,\n",
    "            meta_info   BLOB,\n",
    "            sentence    BLOB,\n",
    "            chunk       BLOB,\n",
    "            token       BLOB\n",
    "        )''')\n",
    "\n",
    "# データをインサートする\n",
    "def load(values):\n",
    "    # exucutemany()はvaluesに指定したパラメータ順序またはマッピングを?に入れて実行できる\n",
    "    conn.executemany('INSERT INTO docs (content, meta_info) VALUES (?,?)', values)\n",
    "    # 確定\n",
    "    conn.commit()\n",
    "\n",
    "# 一部のデータを見る\n",
    "def get(doc_id, fl):\n",
    "    #.fetchone()で指定した1行を取得\n",
    "    row_ls = conn.execute(f\"SELECT {','.join(fl)} FROM docs WHERE id = {doc_id}\").fetchone()\n",
    "    # row_ls = conn.execute('SELECT {} FROM docs WHERE id = ?'.format(','.join(fl)),(doc_id,)).fetchone()\n",
    "    row_dict = {}\n",
    "    # flとrow_lsで抜き出したデータをzipする\n",
    "    for key, value in zip(fl, row_ls):\n",
    "        row_dict[key] = value\n",
    "    return row_dict\n",
    "\n",
    "# id番号を抜き出す\n",
    "def get_all_ids(limit, offset=0):\n",
    "    return [record[0] for record in\n",
    "            # limitで取得上限、OFFSETで開始位置を指定してデータを抜き出す。そのデータの1番目id番号を抜き出す\n",
    "            conn.execute(f'SELECT id FROM docs LIMIT {limit} OFFSET {offset}')]\n",
    "            # conn.execute('SELECT id FROM docs LIMIT ? OFFSET ?',(limit, offset))]\n",
    "\n",
    "def set_annotation(doc_id, name, value):\n",
    "    conn.execute(f'UPDATE docs SET {name} = {json.dumps(value)} where id = {doc_id}')\n",
    "    conn.commit()\n",
    "\n",
    "# アノテーションを取得\n",
    "def get_annotation(doc_id, name):\n",
    "    # docsのid行をwhere idで指定しnameから取り出す\n",
    "    row = conn.execute(f'SELECT {name} FROM docs WHERE id = {doc_id}').fetchone()\n",
    "    if row[0] is not None:\n",
    "        return json.loads(row[0])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# データベースに接続\n",
    "connect()\n",
    "\n",
    "data = []\n",
    "doc_ids = []\n",
    "for doc_id in get_all_ids(limit = -1):\n",
    "    # 文書ごとに出現する単語の原型を取り出して、joinを使用し空白区切りでdata変数にappendで格納していく\n",
    "    data.append(' '.join(\n",
    "        [token['lemma'] for token in get_annotation(doc_id, 'token')]))\n",
    "    # 文書IDを保存しておく\n",
    "    doc_ids.append(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizerを使用し分析開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-4-239f80f8a682>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-239f80f8a682>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "# TfidVectorizerを読み込み\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TFIDFを計算するためにインスタンス化、analyzerは分析対象が単語なのかスペース区切りの文章なのかを指定する。max_dfは全文書中の何割以上の単語を無視するか？\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', max_df=0.9)\n",
    "vecs = vectorizer.fit_transform(data)\n",
    "\n",
    "print(vecs.head())\n",
    "\n",
    "for doc_id, vec in zip(doc_ids, vecs.toarray()):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
