{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CaboChaの出力結果をsqlite3に保存する\n",
    "分解結果をsqlite3に保存する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データベース接続の関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = None\n",
    "\n",
    "# データベース接続\n",
    "def connect():\n",
    "    # global変数でconnを呼び出し\n",
    "    global conn\n",
    "    # データベースの場所を指定\n",
    "    conn = sqlite3.connect('../../NaturalLanguageProcessing/data/sqlite3/sqlite3')\n",
    "\n",
    "# データベース接続終了\n",
    "def close():\n",
    "#   終了\n",
    "    conn.close()\n",
    "\n",
    "# テーブル作成\n",
    "def create_table():\n",
    "#   executeでSQL構文作成、docsがあれば削除\n",
    "    conn.execute('DROP TABLE IF EXISTS docs')\n",
    "#   docsテーブルを新規作成\n",
    "    conn.execute('''CREATE TABLE docs (\n",
    "            id          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            content     TEXT,\n",
    "            meta_info   BLOB,\n",
    "            sentence    BLOB,\n",
    "            chunk       BLOB,\n",
    "            token       BLOB\n",
    "        )''')\n",
    "\n",
    "# データをインサートする\n",
    "def db_load(values):\n",
    "    \n",
    "#   valuesに指定したパラメータ順序またはマッピングを?に入れて実行する\n",
    "    conn.executemany(\n",
    "        'INSERT INTO docs (content, meta_info) VALUES (?,?)',\n",
    "        values)\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# 一部のデータを見る\n",
    "def get(doc_id, fl):\n",
    "#   .fetchone()でカーソルの次の行を取得\n",
    "    row_ls = conn.execute(f\"SELECT {','.join(fl)} FROM docs WHERE id = {doc_id}\").fetchone()\n",
    "    row_dict = {}\n",
    "#   flとrow_lsで抜き出したデータをzipする\n",
    "    for key, value in zip(fl, row_ls):\n",
    "        row_dict[key] = value\n",
    "    return row_dict\n",
    "\n",
    "# id番号を抜き出す\n",
    "def get_all_ids(limit, offset=0):\n",
    "#   limitで取得上限、OFFSETで開始位置を指定してデータを抜き出す。そのデータの1番目id番号を抜き出す\n",
    "    return [record[0] for record in conn.execute('SELECT id FROM docs LIMIT ? OFFSET ?', (limit, offset))]\n",
    "\n",
    "# アノテーションを設定\n",
    "def set_annotation(doc_id, name, value):\n",
    "#   docsのid行をwhere idで指定し、その行にname = valueのアノテーションを追加　\n",
    "    conn.execute('UPDATE docs SET {0} = ? where id = ?'.format(name), (json.dumps(value), doc_id))\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# アノテーションを確認\n",
    "def get_annotation(doc_id, name):\n",
    "#   docsのid行をwhere idで指定しnameから取り出す\n",
    "    row = conn.execute('SELECT {0} FROM docs WHERE id = ?'.format(name), (doc_id,)).fetchone()\n",
    "    if row[0] is not None:\n",
    "        return json.loads(row[0])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabocha処理の関数を定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規表現操作を行うライブラリ\n",
    "import re\n",
    "# CaboChaをインポート\n",
    "import CaboCha\n",
    "\n",
    "# CaboChaをインスタンス化\n",
    "cabocha = CaboCha.Parser('-n1')\n",
    "# 。!<_EOS_>を文の文末としている為正規表現にまとめ文の区切り時に使用する。\n",
    "ptn_sentence = re.compile(r'(^|。|！|<__EOS__>)\\s*(.+?)(?=(。|！|<__EOS__>))', re.M)\n",
    "\n",
    "# 正規表現を使用して文末で分割する関数定義\n",
    "def split_into_sentences(text):\n",
    "    sentences = []\n",
    "    for m in ptn_sentence.finditer(text):\n",
    "        sentences.append((m.group(2), m.start(2)))\n",
    "    return sentences\n",
    "\n",
    "# 係り受け木を構築し、ディキショナリで返す関数定義\n",
    "def parse_sentence(sentence_str, sentence_begin, chunks, tokens):\n",
    "    tree = cabocha.parse(sentence_str)\n",
    "\n",
    "    offset = sentence_begin\n",
    "    chunk_id_offset = len(chunks)\n",
    "    text = sentence_str\n",
    "    for i in range(tree.chunk_size()):\n",
    "        chunk = tree.chunk(i)\n",
    "        chunk_begin = None\n",
    "        for j in range(\n",
    "                chunk.token_pos,\n",
    "                chunk.token_pos + chunk.token_size):\n",
    "            token = tree.token(j)\n",
    "            features = token.feature.split(',')\n",
    "            token_begin = text.find(token.surface) + offset\n",
    "            token_end = token_begin + len(token.surface)\n",
    "            if chunk_begin is None:\n",
    "                chunk_begin = token_begin\n",
    "\n",
    "            tokens.append({\n",
    "                'begin': token_begin,\n",
    "                'end':   token_end,\n",
    "                'lemma': features[-3],\n",
    "                'POS':   features[0],\n",
    "                'POS2':  features[1],\n",
    "                'NE':    token.ne,\n",
    "            })\n",
    "\n",
    "            text = text[token_end-offset:]\n",
    "            offset = token_end\n",
    "\n",
    "        chunk_end = token_end\n",
    "        if chunk.link == -1:\n",
    "            link = -1\n",
    "        else:\n",
    "            # チャンクのIDを手前に出現したチャンス数だけずらす\n",
    "            link = chunk.link + chunk_id_offset\n",
    "        chunks.append({\n",
    "            'begin':    chunk_begin,\n",
    "            'end':      chunk_end,\n",
    "            'link':     ('chunk', link),\n",
    "        })\n",
    "\n",
    "# 上記２つの関数を利用し文の情報を生成する関数定義\n",
    "def parse(text):\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    tokens = []\n",
    "    sentence_begin = 0\n",
    "\n",
    "    # 正規表現を使用して文末で分割\n",
    "    for sentence_str, sentence_begin in split_into_sentences(text):\n",
    "        # 係り受け木を構築し、ディキショナリで返す\n",
    "        parse_sentence(sentence_str, sentence_begin, chunks, tokens)\n",
    "        sentence_end = chunks[-1]['end']\n",
    "\n",
    "        sentences.append({\n",
    "            'begin':    sentence_begin,\n",
    "            'end':      sentence_end,\n",
    "        })\n",
    "\n",
    "    return sentences, chunks, tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqlite3にデータを保存する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docsテーブル確認\n",
    " * ディレクトリ：../../NaturalLanguageProcessing/data/sqlite3/sqlite3\n",
    " * テーブル名：docs\n",
    "\n",
    "| #  | 項目                                      | 型                                   | 項目説明                                 |\n",
    "| -- | ----------------------------------------- | ------------------------------------ | -----------------------------------------|\n",
    "| 1  | id                                        | INTEGER                              | 既に保存                                 |\n",
    "| 2  | content                                   | TEXT                                 | 既に保存                                 |\n",
    "| 3  | meta_info                                 | BLOB                                 | 既に保存                                 |\n",
    "| 4  | <font color=\"red\"><b>sentence</b></font>  | <font color=\"red\"><b>BLOB</b></font> | <font color=\"red\"><b>今回追加</b></font> |\n",
    "| 5  | <font color=\"red\"><b>chunk</b></font>     | <font color=\"red\"><b>BLOB</b></font> | <font color=\"red\"><b>今回追加</b></font> |\n",
    "| 6  | <font color=\"red\"><b>token</b></font>     | <font color=\"red\"><b>BLOB</b></font> | <font color=\"red\"><b>今回追加</b></font> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed: doc_id = 1\n",
      "parsed: doc_id = 2\n",
      "parsed: doc_id = 3\n",
      "parsed: doc_id = 4\n",
      "parsed: doc_id = 5\n",
      "parsed: doc_id = 6\n",
      "parsed: doc_id = 7\n",
      "parsed: doc_id = 8\n",
      "parsed: doc_id = 9\n",
      "parsed: doc_id = 10\n",
      "parsed: doc_id = 11\n",
      "parsed: doc_id = 12\n",
      "parsed: doc_id = 13\n",
      "parsed: doc_id = 14\n",
      "parsed: doc_id = 15\n",
      "parsed: doc_id = 16\n",
      "parsed: doc_id = 17\n",
      "parsed: doc_id = 18\n",
      "parsed: doc_id = 19\n",
      "parsed: doc_id = 20\n",
      "parsed: doc_id = 21\n",
      "parsed: doc_id = 22\n",
      "parsed: doc_id = 23\n",
      "parsed: doc_id = 24\n",
      "parsed: doc_id = 25\n",
      "parsed: doc_id = 26\n",
      "parsed: doc_id = 27\n",
      "parsed: doc_id = 28\n",
      "parsed: doc_id = 29\n",
      "parsed: doc_id = 30\n",
      "parsed: doc_id = 31\n",
      "parsed: doc_id = 32\n",
      "parsed: doc_id = 33\n",
      "parsed: doc_id = 34\n",
      "parsed: doc_id = 35\n",
      "parsed: doc_id = 36\n",
      "parsed: doc_id = 37\n",
      "parsed: doc_id = 38\n",
      "parsed: doc_id = 39\n",
      "parsed: doc_id = 40\n",
      "parsed: doc_id = 41\n",
      "parsed: doc_id = 42\n",
      "parsed: doc_id = 43\n",
      "parsed: doc_id = 44\n",
      "parsed: doc_id = 45\n",
      "parsed: doc_id = 46\n",
      "parsed: doc_id = 47\n",
      "parsed: doc_id = 48\n",
      "parsed: doc_id = 49\n",
      "parsed: doc_id = 50\n",
      "parsed: doc_id = 51\n",
      "parsed: doc_id = 52\n",
      "parsed: doc_id = 53\n",
      "parsed: doc_id = 54\n",
      "parsed: doc_id = 55\n",
      "parsed: doc_id = 56\n",
      "parsed: doc_id = 57\n",
      "parsed: doc_id = 58\n",
      "parsed: doc_id = 59\n",
      "parsed: doc_id = 60\n",
      "parsed: doc_id = 61\n",
      "parsed: doc_id = 62\n",
      "parsed: doc_id = 63\n",
      "parsed: doc_id = 64\n",
      "parsed: doc_id = 65\n",
      "parsed: doc_id = 66\n",
      "parsed: doc_id = 67\n",
      "parsed: doc_id = 68\n",
      "parsed: doc_id = 69\n",
      "parsed: doc_id = 70\n",
      "parsed: doc_id = 71\n",
      "parsed: doc_id = 72\n",
      "parsed: doc_id = 73\n",
      "parsed: doc_id = 74\n",
      "parsed: doc_id = 75\n",
      "parsed: doc_id = 76\n",
      "parsed: doc_id = 77\n",
      "parsed: doc_id = 78\n",
      "parsed: doc_id = 79\n",
      "parsed: doc_id = 80\n",
      "parsed: doc_id = 81\n",
      "parsed: doc_id = 82\n",
      "parsed: doc_id = 83\n",
      "parsed: doc_id = 84\n",
      "parsed: doc_id = 85\n",
      "parsed: doc_id = 86\n",
      "parsed: doc_id = 87\n",
      "parsed: doc_id = 88\n",
      "parsed: doc_id = 89\n",
      "parsed: doc_id = 90\n",
      "parsed: doc_id = 91\n",
      "parsed: doc_id = 92\n",
      "parsed: doc_id = 93\n",
      "parsed: doc_id = 94\n",
      "parsed: doc_id = 95\n",
      "parsed: doc_id = 96\n",
      "parsed: doc_id = 97\n",
      "parsed: doc_id = 98\n",
      "parsed: doc_id = 99\n",
      "parsed: doc_id = 100\n",
      "parsed: doc_id = 101\n",
      "parsed: doc_id = 102\n",
      "parsed: doc_id = 103\n",
      "parsed: doc_id = 104\n",
      "parsed: doc_id = 105\n",
      "parsed: doc_id = 106\n",
      "parsed: doc_id = 107\n",
      "parsed: doc_id = 108\n",
      "parsed: doc_id = 109\n",
      "parsed: doc_id = 110\n",
      "parsed: doc_id = 111\n",
      "parsed: doc_id = 112\n",
      "parsed: doc_id = 113\n",
      "parsed: doc_id = 114\n",
      "parsed: doc_id = 115\n",
      "parsed: doc_id = 116\n",
      "parsed: doc_id = 117\n",
      "parsed: doc_id = 118\n",
      "parsed: doc_id = 119\n",
      "parsed: doc_id = 120\n",
      "parsed: doc_id = 121\n",
      "parsed: doc_id = 122\n",
      "parsed: doc_id = 123\n",
      "parsed: doc_id = 124\n",
      "parsed: doc_id = 125\n",
      "parsed: doc_id = 126\n",
      "parsed: doc_id = 127\n",
      "parsed: doc_id = 128\n",
      "parsed: doc_id = 129\n",
      "parsed: doc_id = 130\n",
      "parsed: doc_id = 131\n",
      "parsed: doc_id = 132\n",
      "parsed: doc_id = 133\n",
      "parsed: doc_id = 134\n",
      "parsed: doc_id = 135\n",
      "parsed: doc_id = 136\n",
      "parsed: doc_id = 137\n",
      "parsed: doc_id = 138\n",
      "parsed: doc_id = 139\n",
      "parsed: doc_id = 140\n",
      "parsed: doc_id = 141\n",
      "parsed: doc_id = 142\n",
      "parsed: doc_id = 143\n",
      "parsed: doc_id = 144\n",
      "parsed: doc_id = 145\n",
      "parsed: doc_id = 146\n",
      "parsed: doc_id = 147\n",
      "parsed: doc_id = 148\n",
      "parsed: doc_id = 149\n",
      "parsed: doc_id = 150\n",
      "parsed: doc_id = 151\n",
      "parsed: doc_id = 152\n",
      "parsed: doc_id = 153\n",
      "parsed: doc_id = 154\n",
      "parsed: doc_id = 155\n",
      "parsed: doc_id = 156\n",
      "parsed: doc_id = 157\n",
      "parsed: doc_id = 158\n",
      "parsed: doc_id = 159\n",
      "parsed: doc_id = 160\n",
      "parsed: doc_id = 161\n",
      "parsed: doc_id = 162\n",
      "parsed: doc_id = 163\n",
      "parsed: doc_id = 164\n",
      "parsed: doc_id = 165\n",
      "parsed: doc_id = 166\n",
      "parsed: doc_id = 167\n",
      "parsed: doc_id = 168\n",
      "parsed: doc_id = 169\n",
      "parsed: doc_id = 170\n",
      "parsed: doc_id = 171\n",
      "parsed: doc_id = 172\n",
      "parsed: doc_id = 173\n",
      "parsed: doc_id = 174\n",
      "parsed: doc_id = 175\n",
      "parsed: doc_id = 176\n",
      "parsed: doc_id = 177\n",
      "parsed: doc_id = 178\n",
      "parsed: doc_id = 179\n",
      "parsed: doc_id = 180\n",
      "parsed: doc_id = 181\n",
      "parsed: doc_id = 182\n",
      "parsed: doc_id = 183\n",
      "parsed: doc_id = 184\n",
      "parsed: doc_id = 185\n",
      "parsed: doc_id = 186\n",
      "parsed: doc_id = 187\n",
      "parsed: doc_id = 188\n",
      "parsed: doc_id = 189\n",
      "parsed: doc_id = 190\n",
      "parsed: doc_id = 191\n",
      "parsed: doc_id = 192\n",
      "parsed: doc_id = 193\n",
      "parsed: doc_id = 194\n",
      "parsed: doc_id = 195\n",
      "parsed: doc_id = 196\n",
      "parsed: doc_id = 197\n"
     ]
    }
   ],
   "source": [
    "connect()\n",
    "for doc_id in get_all_ids(limit=-1):\n",
    "    row = get(doc_id, fl=['content'])\n",
    "    text = row['content']\n",
    "    sentences, chunks, tokens = parse(text)\n",
    "    print('parsed: doc_id =', doc_id)\n",
    "\n",
    "    set_annotation(doc_id, 'sentence', sentences)\n",
    "    set_annotation(doc_id, 'chunk',    chunks)\n",
    "    set_annotation(doc_id, 'token',    tokens)\n",
    "\n",
    "close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
