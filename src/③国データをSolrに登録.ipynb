{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### スクレイピングとクレンジング処理関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# translateで指定する変換法則を指定\n",
    "translation_table = str.maketrans(dict(zip('()!', '（）！')))\n",
    "\n",
    "# クレンジング処理\n",
    "def cleanse(text):\n",
    "#   NFKC形式でtextを変換、さらに、translateにtranslation_tableを指定することでdict内に従って変換する\n",
    "    text = unicodedata.normalize('NFKC', text).translate(translation_table)\n",
    "#   正規表現を使用し\\s+（ひとつ以上の空白文字の連続）を空白一つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# スクレイピング処理\n",
    "def scrape(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # __EOS__ の挿入\n",
    "    for block in soup.find_all(['br', 'p', 'h1', 'h2', 'h3', 'h4']):\n",
    "        # strip()で改行を取り除いたブロック内テキストの文字数が0以上 かつ 後ろから一文字目が'。', '！'でない場合\n",
    "        if len(block.text.strip()) > 0 and block.text.strip()[-1] not in ['。', '！']:\n",
    "            block.append('<__EOS__>')\n",
    "            \n",
    "    # 本文の抽出\n",
    "    text = '\\n'.join([\n",
    "        # クレンジング処理を行う\n",
    "        cleanse(block.text.strip()) for block in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4']) if len(block.text.strip()) > 0])\n",
    "    \n",
    "    # タイトルの抽出\n",
    "    # ' - Wikipedia'を削除しcleanse\n",
    "    title = cleanse(soup.title.text.replace(' - Wikipedia', ''))\n",
    "    \n",
    "    return text, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### DB操作関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = None\n",
    "\n",
    "# データベース接続\n",
    "def connect():\n",
    "    # global変数でconnを呼び出し\n",
    "    global conn\n",
    "    # データベースの場所を指定\n",
    "    conn = sqlite3.connect('../data/sqlite3/sqlite3')\n",
    "\n",
    "# データベース接続終了\n",
    "def close():\n",
    "#   終了\n",
    "    conn.close()\n",
    "\n",
    "# テーブル作成\n",
    "def create_table():\n",
    "#   executeでSQL構文作成、docsがあれば削除\n",
    "    conn.execute('DROP TABLE IF EXISTS docs')\n",
    "#   docsテーブルを新規作成\n",
    "    conn.execute('''CREATE TABLE docs (\n",
    "            id          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            content     TEXT,\n",
    "            meta_info   BLOB,\n",
    "            sentence    BLOB,\n",
    "            chunk       BLOB,\n",
    "            token       BLOB\n",
    "        )''')\n",
    "\n",
    "# データをインサートする\n",
    "def db_load(values):\n",
    "    \n",
    "#   valuesに指定したパラメータ順序またはマッピングを?に入れて実行する\n",
    "    conn.executemany(\n",
    "        'INSERT INTO docs (content, meta_info) VALUES (?,?)',\n",
    "        values)\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# 一部のデータを見る\n",
    "def get(doc_id, fl):\n",
    "#   .fetchone()でカーソルの次の行を取得\n",
    "    row_ls = conn.execute(f\"SELECT {','.join(fl)} FROM docs WHERE id = {doc_id}\").fetchone()\n",
    "    row_dict = {}\n",
    "#   flとrow_lsで抜き出したデータをzipする\n",
    "    for key, value in zip(fl, row_ls):\n",
    "        row_dict[key] = value\n",
    "    return row_dict\n",
    "\n",
    "# id番号を抜き出す\n",
    "def get_all_ids(limit, offset=0):\n",
    "#   limitで取得上限、OFFSETで開始位置を指定してデータを抜き出す。そのデータの1番目id番号を抜き出す\n",
    "    return [record[0] for record in conn.execute('SELECT id FROM docs LIMIT ? OFFSET ?', (limit, offset))]\n",
    "\n",
    "# アノテーションを設定\n",
    "def set_annotation(doc_id, name, value):\n",
    "#   docsのid行をwhere idで指定し、その行にname = valueのアノテーションを追加　\n",
    "    conn.execute('UPDATE docs SET {0} = ? where id = ?'.format(name), (json.dumps(value), doc_id))\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# アノテーションを確認\n",
    "def get_annotation(doc_id, name):\n",
    "#   docsのid行をwhere idで指定しnameから取り出す\n",
    "    row = conn.execute('SELECT {0} FROM docs WHERE id = ?'.format(name), (doc_id,)).fetchone()\n",
    "    if row[0] is not None:\n",
    "        return json.loads(row[0])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### solrに登録、検索、アノテーション付関数定義　-searchから未解読"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 使用するSolrのURL\n",
    "solr_url = 'http://localhost:8983/solr'\n",
    "# build_openerはログインが必要なサイトのときに使用する\n",
    "opener = urllib.request.build_opener(urllib.request.ProxyHandler())\n",
    "\n",
    "# Solrにデータを登録する関数、引数dataは登録するdataをdictで指定\n",
    "def solr_load(collection, data):\n",
    "    \n",
    "    # Solrのコアに対してデータを登録するリクエストを作成,collectionにはデータ登録先のコア名を指定している\n",
    "    url='{0}/{1}/update'.format(solr_url, collection)\n",
    "    # Requestインスタンスの作成,\n",
    "    req = urllib.request.Request(\n",
    "        url,\n",
    "        # dataをdumps()でutf-8にエンコード\n",
    "        data=json.dumps(data).encode('utf-8'),\n",
    "        headers={'content-type': 'application/json'})\n",
    "\n",
    "    # データの登録を実行\n",
    "    print(url)\n",
    "    # resでリクエストの返答を受け取る\n",
    "    with opener.open(req) as res:\n",
    "        # データ確認\n",
    "        print(res.read().decode('utf-8'))\n",
    "\n",
    "    # Solrのコアに対してコミット指示するリクエストを作成,collectionにはデータ登録先のコア名を指定している\n",
    "    url = '{0}/{1}/update?softCommit=true'.format(solr_url, collection)\n",
    "    # urlに対してリクエスト\n",
    "    req = urllib.request.Request(url)\n",
    "    # resuでリクエストの返答を受け取る、opnerはプロキシ環境変数に設定している場合も動くようにする為\n",
    "    with opener.open(req) as res:\n",
    "        # データを確認\n",
    "        print(res.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "def search(keywords, rows=100):\n",
    "    query = ' AND '.join([\n",
    "        '(' + ' OR '.join(['content_txt_ja:\"{}\"'.format(keyword)\n",
    "                           for keyword in group]) + ')'\n",
    "        for group in keywords])\n",
    "    data = {\n",
    "        'q':     query,\n",
    "        'wt':    'json',\n",
    "        'rows':  rows,\n",
    "        'hl':    'on',\n",
    "        'hl.fl': 'content_txt_ja',\n",
    "    }\n",
    "    # 検索リクエストの作成（＊１）\n",
    "    req = urllib.request.Request(\n",
    "        url='{}/doc/select'.format(solr_url),\n",
    "        data=urllib.parse.urlencode(data).encode('utf-8'),\n",
    "    )\n",
    "    # 検索リクエストの実行（＊２）\n",
    "    with opener.open(req) as res:\n",
    "        return json.loads(res.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "def search_annotation(fl_keyword_pairs, rows=100):\n",
    "    query = ' AND '.join([\n",
    "        '(' + ' OR '.join(['{0}:\"{1}\"'.format(fl, keyword)\n",
    "                           for keyword in group]) + ')'\n",
    "        for fl, keywords in fl_keyword_pairs\n",
    "            for group in keywords])\n",
    "    data = {\n",
    "        'q':    query,\n",
    "        'wt':   'json',\n",
    "        'rows': rows,\n",
    "    }\n",
    "    # 検索リクエストの作成（＊１）\n",
    "    req = urllib.request.Request(\n",
    "        url='{}/anno/select'.format(solr_url),\n",
    "        data=urllib.parse.urlencode(data).encode('utf-8'),\n",
    "    )\n",
    "    # 検索リクエストの実行（＊２）\n",
    "    with opener.open(req) as res:\n",
    "        return json.loads(res.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### テーブル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースの関数を実行する\n",
    "connect()\n",
    "create_table()\n",
    "close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### DBにwikipediaデータを登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped finish\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# 引数に指定されたパターンにマッチするファイルパス名を取得するモジュール\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "values = []\n",
    "# 引数に指定されたパターンにマッチするファイルパス名を取得しfor文で回す\n",
    "for filename in glob.glob('../data/wikipedia/*.html'):\n",
    "    with open(filename) as fin:\n",
    "        # filenameの読み込み\n",
    "        html = fin.read()\n",
    "        # スクレイプ関数でスクレイピング処理\n",
    "        text, title = scrape(html)\n",
    "        # 下記でスクレイピング処理の確認\n",
    "        # print('scraped:', title)\n",
    "        url = 'https://ja.wikipedia.org/wiki/{0}'.format(urllib.parse.quote(title))\n",
    "        values.append((text, json.dumps({'url': url, 'title': title})))\n",
    "\n",
    "print('scraped finish')\n",
    "connect()\n",
    "db_load(values)\n",
    "\n",
    "print(list(get_all_ids(limit=-1)))\n",
    "close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Solrにコアを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n",
      "         To turn off: bin/solr config -c doc -p 8983 -action set-user-property -property update.autoCreateFields -value false\n",
      "\n",
      "ERROR: \n",
      "Core 'doc' already exists!\n",
      "Checked core existence using Core API command:\n",
      "http://localhost:8983/solr/admin/cores?action=STATUS&core=doc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! /vagrant/NaturalLanguageProcessing/solr-8.2.0/bin/solr create -c doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### spliteデータをsolrに格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:8983/solr/doc/update\n",
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":3482}}\n",
      "\n",
      "{\n",
      "  \"responseHeader\":{\n",
      "    \"status\":0,\n",
      "    \"QTime\":832}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# データベースに接続\n",
    "connect()\n",
    "data = []\n",
    "# id番号を抜き出し回す\n",
    "for doc_id in get_all_ids(limit=-1):\n",
    "    row = get(doc_id, ['id', 'content', 'meta_info'])\n",
    "    # Solr へ登録するデータ構造へ変換\n",
    "    meta_info = json.loads(row['meta_info'])\n",
    "    data.append({\n",
    "        'id':               str(row['id']),\n",
    "        'doc_id_i':         row['id'],\n",
    "        'content_txt_ja':   row['content'],\n",
    "        'title_txt_ja':     meta_info['title'],\n",
    "        'url_s':            meta_info['url'],\n",
    "    })\n",
    "# Solr への登録を実行\n",
    "solr_load('doc', data)\n",
    "close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solrにアクセスする\n",
    "* ブラウザで下記にアクセス\n",
    "```\n",
    "http://localhost:8983/solr\n",
    "```\n",
    "* 「Core Selector」-「doc」を選択\n",
    "* 「query」を選択\n",
    "* qの欄に「content_txt_ja:\"石油ガス\" and content_txt_ja:\"天然ガス\"」\n",
    "* Execute Queryを実行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
