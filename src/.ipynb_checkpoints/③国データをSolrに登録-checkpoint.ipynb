{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### スクレイピングとクレンジング処理関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# translateで指定する変換法則を指定\n",
    "translation_table = str.maketrans(dict(zip('()!', '（）！')))\n",
    "\n",
    "# クレンジング処理\n",
    "def cleanse(text):\n",
    "#   NFKC形式でtextを変換、さらに、translateにtranslation_tableを指定することでdict内に従って変換する\n",
    "    text = unicodedata.normalize('NFKC', text).translate(translation_table)\n",
    "#   正規表現を使用し\\s+（ひとつ以上の空白文字の連続）を空白一つに変換\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# スクレイピング処理\n",
    "def scrape(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # __EOS__ の挿入\n",
    "    for block in soup.find_all(['br', 'p', 'h1', 'h2', 'h3', 'h4']):\n",
    "        # strip()で改行を取り除いたブロック内テキストの文字数が0以上 かつ 後ろから一文字目が'。', '！'でない場合\n",
    "        if len(block.text.strip()) > 0 and block.text.strip()[-1] not in ['。', '！']:\n",
    "            block.append('<__EOS__>')\n",
    "            \n",
    "    # 本文の抽出\n",
    "    text = '\\n'.join([\n",
    "        # クレンジング処理を行う\n",
    "        cleanse(block.text.strip()) for block in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4']) if len(block.text.strip()) > 0])\n",
    "    \n",
    "    # タイトルの抽出\n",
    "    # ' - Wikipedia'を削除しcleanse\n",
    "    title = cleanse(soup.title.text.replace(' - Wikipedia', ''))\n",
    "    \n",
    "    return text, title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### DB操作関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "conn = None\n",
    "\n",
    "# データベース接続\n",
    "def connect():\n",
    "    # global変数でconnを呼び出し\n",
    "    global conn\n",
    "    # データベースの場所を指定\n",
    "    conn = sqlite3.connect('../data/sqlite3/sqlite3')\n",
    "\n",
    "# データベース接続終了\n",
    "def close():\n",
    "#   終了\n",
    "    conn.close()\n",
    "\n",
    "# テーブル作成\n",
    "def create_table():\n",
    "#   executeでSQL構文作成、docsがあれば削除\n",
    "    conn.execute('DROP TABLE IF EXISTS docs')\n",
    "#   docsテーブルを新規作成\n",
    "    conn.execute('''CREATE TABLE docs (\n",
    "            id          INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            content     TEXT,\n",
    "            meta_info   BLOB,\n",
    "            sentence    BLOB,\n",
    "            chunk       BLOB,\n",
    "            token       BLOB\n",
    "        )''')\n",
    "\n",
    "# データをインサートする\n",
    "def db_load(values):\n",
    "    \n",
    "#   valuesに指定したパラメータ順序またはマッピングを?に入れて実行する\n",
    "    conn.executemany(\n",
    "        'INSERT INTO docs (content, meta_info) VALUES (?,?)',\n",
    "        values)\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# 一部のデータを見る\n",
    "def get(doc_id, fl):\n",
    "#   .fetchone()でカーソルの次の行を取得\n",
    "    row_ls = conn.execute(f\"SELECT {','.join(fl)} FROM docs WHERE id = {doc_id}\").fetchone()\n",
    "    row_dict = {}\n",
    "#   flとrow_lsで抜き出したデータをzipする\n",
    "    for key, value in zip(fl, row_ls):\n",
    "        row_dict[key] = value\n",
    "    return row_dict\n",
    "\n",
    "# id番号を抜き出す\n",
    "def get_all_ids(limit, offset=0):\n",
    "#   limitで取得上限、OFFSETで開始位置を指定してデータを抜き出す。そのデータの1番目id番号を抜き出す\n",
    "    return [record[0] for record in conn.execute('SELECT id FROM docs LIMIT ? OFFSET ?', (limit, offset))]\n",
    "\n",
    "# アノテーションを設定\n",
    "def set_annotation(doc_id, name, value):\n",
    "#   docsのid行をwhere idで指定し、その行にname = valueのアノテーションを追加　\n",
    "    conn.execute('UPDATE docs SET {0} = ? where id = ?'.format(name), (json.dumps(value), doc_id))\n",
    "#   確定\n",
    "    conn.commit()\n",
    "\n",
    "# アノテーションを確認\n",
    "def get_annotation(doc_id, name):\n",
    "#   docsのid行をwhere idで指定しnameから取り出す\n",
    "    row = conn.execute('SELECT {0} FROM docs WHERE id = ?'.format(name), (doc_id,)).fetchone()\n",
    "    if row[0] is not None:\n",
    "        return json.loads(row[0])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### solrに登録、検索、アノテーション付関数定義　-searchから未解読"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# 使用するSolrのURL\n",
    "solr_url = 'http://localhost:8983/solr'\n",
    "# build_openerはログインが必要なサイトのときに使用する\n",
    "opener = urllib.request.build_opener(urllib.request.ProxyHandler())\n",
    "\n",
    "# Solrにデータを登録する関数、引数dataは登録するdataをdictで指定\n",
    "def solr_load(collection, data):\n",
    "    \n",
    "    # Solrのコアに対してデータを登録するリクエストを作成,collectionにはデータ登録先のコア名を指定している\n",
    "    url='{0}/{1}/update'.format(solr_url, collection)\n",
    "    # Requestインスタンスの作成,\n",
    "    req = urllib.request.Request(\n",
    "        url,\n",
    "        # dataをdumps()でutf-8にエンコード\n",
    "        data=json.dumps(data).encode('utf-8'),\n",
    "        headers={'content-type': 'application/json'})\n",
    "\n",
    "    # データの登録を実行\n",
    "    print(url)\n",
    "    # resでリクエストの返答を受け取る\n",
    "    with opener.open(req) as res:\n",
    "        # データ確認\n",
    "        print(res.read().decode('utf-8'))\n",
    "\n",
    "    # Solrのコアに対してコミット指示するリクエストを作成,collectionにはデータ登録先のコア名を指定している\n",
    "    url = '{0}/{1}/update?softCommit=true'.format(solr_url, collection)\n",
    "    # urlに対してリクエスト\n",
    "    req = urllib.request.Request(url)\n",
    "    # resuでリクエストの返答を受け取る、opnerはプロキシ環境変数に設定している場合も動くようにする為\n",
    "    with opener.open(req) as res:\n",
    "        # データを確認\n",
    "        print(res.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "def search(keywords, rows=100):\n",
    "    query = ' AND '.join([\n",
    "        '(' + ' OR '.join(['content_txt_ja:\"{}\"'.format(keyword)\n",
    "                           for keyword in group]) + ')'\n",
    "        for group in keywords])\n",
    "    data = {\n",
    "        'q':     query,\n",
    "        'wt':    'json',\n",
    "        'rows':  rows,\n",
    "        'hl':    'on',\n",
    "        'hl.fl': 'content_txt_ja',\n",
    "    }\n",
    "    # 検索リクエストの作成（＊１）\n",
    "    req = urllib.request.Request(\n",
    "        url='{}/doc/select'.format(solr_url),\n",
    "        data=urllib.parse.urlencode(data).encode('utf-8'),\n",
    "    )\n",
    "    # 検索リクエストの実行（＊２）\n",
    "    with opener.open(req) as res:\n",
    "        return json.loads(res.read().decode('utf-8'))\n",
    "\n",
    "\n",
    "def search_annotation(fl_keyword_pairs, rows=100):\n",
    "    query = ' AND '.join([\n",
    "        '(' + ' OR '.join(['{0}:\"{1}\"'.format(fl, keyword)\n",
    "                           for keyword in group]) + ')'\n",
    "        for fl, keywords in fl_keyword_pairs\n",
    "            for group in keywords])\n",
    "    data = {\n",
    "        'q':    query,\n",
    "        'wt':   'json',\n",
    "        'rows': rows,\n",
    "    }\n",
    "    # 検索リクエストの作成（＊１）\n",
    "    req = urllib.request.Request(\n",
    "        url='{}/anno/select'.format(solr_url),\n",
    "        data=urllib.parse.urlencode(data).encode('utf-8'),\n",
    "    )\n",
    "    # 検索リクエストの実行（＊２）\n",
    "    with opener.open(req) as res:\n",
    "        return json.loads(res.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### テーブル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データベースの関数を実行する\n",
    "connect()\n",
    "create_table()\n",
    "close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### DBにwikipediaデータを登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped: ボリビア\n",
      "scraped: ブルネイ\n",
      "scraped: ブークモール\n",
      "scraped: ブータン\n",
      "scraped: チェコ\n",
      "scraped: シリア\n",
      "scraped: ブルガリア\n",
      "scraped: セルビア\n",
      "scraped: チリ\n",
      "scraped: ガーナ\n",
      "scraped: タイ王国\n",
      "scraped: エクアドル\n",
      "scraped: フィジー\n",
      "scraped: モルドバ\n",
      "scraped: パキスタン\n",
      "scraped: ルーマニア\n",
      "scraped: マルタ\n",
      "scraped: ケニア\n",
      "scraped: オーストラリア\n",
      "scraped: アンドラ\n",
      "scraped: グアテマラ\n",
      "scraped: 大韓民国\n",
      "scraped: スリランカ\n",
      "scraped: タンザニア\n",
      "scraped: イスラエル\n",
      "scraped: ベネズエラ\n",
      "scraped: ボスニア・ヘルツェゴビナ\n",
      "scraped: ハイチ\n",
      "scraped: カーボベルデ\n",
      "scraped: ブラジル\n",
      "scraped: バングラデシュ\n",
      "scraped: マッキューン=ライシャワー式\n",
      "scraped: アンティグア・バーブーダ\n",
      "scraped: ベラルーシ\n",
      "scraped: スーダン\n",
      "scraped: コロンビア\n",
      "scraped: アルバニア\n",
      "scraped: エスワティニ\n",
      "scraped: 赤道ギニア\n",
      "scraped: スイス\n",
      "scraped: トルコ\n",
      "scraped: アフガニスタン\n",
      "scraped: アイルランド\n",
      "scraped: アルジェリア\n",
      "scraped: コスタリカ\n",
      "scraped: コンゴ民主共和国\n",
      "scraped: コンゴ共和国\n",
      "scraped: バーレーン\n",
      "scraped: バハマ\n",
      "scraped: モーリタニア\n",
      "scraped: ハンガリー\n",
      "scraped: カンボジア\n",
      "scraped: メキシコ\n",
      "scraped: 朝鮮民主主義人民共和国\n",
      "scraped: 日本\n",
      "scraped: バルバドス\n",
      "scraped: ニジェール\n",
      "scraped: アルメニア\n",
      "scraped: ツバル\n",
      "scraped: コートジボワール\n",
      "scraped: カメルーン\n",
      "scraped: 南アフリカ共和国\n",
      "scraped: ナミビア\n",
      "scraped: アメリカ合衆国\n",
      "scraped: ザンビア\n",
      "scraped: レソト\n",
      "scraped: モーリシャス\n",
      "scraped: タジキスタン\n",
      "scraped: マリ共和国\n",
      "scraped: アイスランド\n",
      "scraped: オマーン\n",
      "scraped: モナコ\n",
      "scraped: ナイジェリア\n",
      "scraped: カザフスタン\n",
      "scraped: ロシア\n",
      "scraped: ギリシャ\n",
      "scraped: ナウル\n",
      "scraped: ルクセンブルク\n",
      "scraped: ポーランド\n",
      "scraped: マダガスカル\n",
      "scraped: カナダ\n",
      "scraped: スリナム\n",
      "scraped: ガンビア\n",
      "scraped: トンガ\n",
      "scraped: トリニダード・トバゴ\n",
      "scraped: ニカラグア\n",
      "scraped: デンマーク\n",
      "scraped: チャド\n",
      "scraped: アゼルバイジャン\n",
      "scraped: パプアニューギニア\n",
      "scraped: フィリピン\n",
      "scraped: オランダ\n",
      "scraped: ウルグアイ\n",
      "scraped: チュニジア\n",
      "scraped: キプロス\n",
      "scraped: スロバキア\n",
      "scraped: リトアニア\n",
      "scraped: グレナダ\n",
      "scraped: ギニアビサウ\n",
      "scraped: エチオピア\n",
      "scraped: トーゴ\n",
      "scraped: パラグアイ\n",
      "scraped: ヨルダン\n",
      "scraped: アンゴラ\n",
      "scraped: レバノン\n",
      "scraped: キリバス\n",
      "scraped: ドミニカ国\n",
      "scraped: コモロ\n",
      "scraped: ウズベキスタン\n",
      "scraped: イタリア\n",
      "scraped: モンゴル国\n",
      "scraped: エジプト\n",
      "scraped: ベトナム\n",
      "scraped: クウェート\n",
      "scraped: ミャンマー\n",
      "scraped: マレーシア\n",
      "scraped: サウジアラビア\n",
      "scraped: ジョージア （国）\n",
      "scraped: ジンバブエ\n",
      "scraped: セネガル\n",
      "scraped: セントルシア\n",
      "scraped: ジャマイカ\n",
      "scraped: スウェーデン\n",
      "scraped: ドミニカ共和国\n",
      "scraped: ベリーズ\n",
      "scraped: ベルギー\n",
      "scraped: ノルウェー\n",
      "scraped: リビア\n",
      "scraped: パラオ\n",
      "scraped: イラク\n",
      "scraped: エリトリア\n",
      "scraped: クロアチア\n",
      "scraped: シンガポール\n",
      "scraped: ソロモン諸島\n",
      "scraped: スペイン\n",
      "scraped: セーシェル\n",
      "scraped: サントメ・プリンシペ\n",
      "scraped: ワイリー方式\n",
      "scraped: エストニア\n",
      "scraped: ニュージーランド\n",
      "scraped: キルギス\n",
      "scraped: 中華人民共和国\n",
      "scraped: マーシャル諸島\n",
      "scraped: マラウイ\n",
      "scraped: モンテネグロ\n",
      "scraped: ネパール\n",
      "scraped: ガイアナ\n",
      "scraped: モロッコ\n",
      "scraped: イラン\n",
      "scraped: セントクリストファー・ネイビス\n",
      "scraped: ポルトガル\n",
      "scraped: ブルンジ\n",
      "scraped: トルクメニスタン\n",
      "scraped: 北マケドニア\n",
      "scraped: ブルキナファソ\n",
      "scraped: ラトビア\n",
      "scraped: モルディブ\n",
      "scraped: ウガンダ\n",
      "scraped: ガボン\n",
      "scraped: イギリス\n",
      "scraped: ソマリア\n",
      "scraped: モザンビーク\n",
      "scraped: セントビンセント・グレナディーン\n",
      "scraped: フィンランド\n",
      "scraped: サンマリノ\n",
      "scraped: ニーノシュク\n",
      "scraped: インドネシア\n",
      "scraped: アルゼンチン\n",
      "scraped: ホンジュラス\n",
      "scraped: カタール\n",
      "scraped: ジブチ\n",
      "scraped: インド\n",
      "scraped: スロベニア\n",
      "scraped: ドイツ\n",
      "scraped: アラブ首長国連邦\n",
      "scraped: ギニア\n",
      "scraped: ウクライナ\n",
      "scraped: リベリア\n",
      "scraped: フランス\n",
      "scraped: ベナン\n",
      "scraped: 南スーダン\n",
      "scraped: 東ティモール\n",
      "scraped: シエラレオネ\n",
      "scraped: エルサルバドル\n",
      "scraped: ペルー\n",
      "scraped: リヒテンシュタイン\n",
      "scraped: ボツワナ\n",
      "scraped: ミクロネシア連邦\n",
      "scraped: 中央アフリカ共和国\n",
      "scraped: イエメン\n",
      "scraped: サモア\n",
      "scraped: ラオス\n",
      "scraped: バヌアツ\n",
      "scraped: パナマ\n",
      "scraped: オーストリア\n",
      "scraped: キューバ\n",
      "scraped: ルワンダ\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# 引数に指定されたパターンにマッチするファイルパス名を取得するモジュール\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "values = []\n",
    "# 引数に指定されたパターンにマッチするファイルパス名を取得しfor文で回す\n",
    "for filename in glob.glob('../data/wikipedia/*.html'):\n",
    "    with open(filename) as fin:\n",
    "        # filenameの読み込み\n",
    "        html = fin.read()\n",
    "        # スクレイプ関数でスクレイピング処理\n",
    "        text, title = scrape(html)\n",
    "        # 下記でスクレイピング処理の確認\n",
    "        # print('scraped:', title)\n",
    "        url = 'https://ja.wikipedia.org/wiki/{0}'.format(urllib.parse.quote(title))\n",
    "        values.append((text, json.dumps({'url': url, 'title': title})))\n",
    "\n",
    "print('scraped finish')\n",
    "connect()\n",
    "db_load(values)\n",
    "\n",
    "print(list(get_all_ids(limit=-1)))\n",
    "close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Solrにコアを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using _default configset with data driven schema functionality. NOT RECOMMENDED for production use.\n",
      "         To turn off: bin/solr config -c doc -p 8983 -action set-user-property -property update.autoCreateFields -value false\n",
      "\n",
      "ERROR: \n",
      "Core 'doc' already exists!\n",
      "Checked core existence using Core API command:\n",
      "http://localhost:8983/solr/admin/cores?action=STATUS&core=doc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! /vagrant/NaturalLanguageProcessing/solr-8.2.0/bin/solr create -c doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### データ格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1658f3ba1319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# collectionは作成したコア名\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/wikipedia/*.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "connect()\n",
    "values = []\n",
    "# collectionは作成したコア名\n",
    "collection = doc\n",
    "for filename in glob.glob('../data/wikipedia/*.html'):\n",
    "    print(filename)\n",
    "    with open(filename,encoding=\"utf-8\") as fin:\n",
    "        html = fin.read()\n",
    "        text, title = scrape(html)\n",
    "        print('scraped:', title)\n",
    "        url = 'https://ja.wikipedia.org/wiki/{0}'.format(\n",
    "            urllib.parse.quote(title))\n",
    "        values.append((text, json.dumps({'url': url, 'title': title})))\n",
    "        \n",
    "# Solrに登録する関数\n",
    "load(collection, values)\n",
    "\n",
    "print(list(get_all_ids(limit=-1)))\n",
    "\n",
    "close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
