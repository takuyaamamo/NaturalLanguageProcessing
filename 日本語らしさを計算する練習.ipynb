{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 日本語らしさを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文の中の単語をランダムに並び替えて、言語モデルを使って「どの程度日本語らしいか」を計算してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデル関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "# MLE(Maximum Likelihood Estimator):言語モデルを作成するライブラリ\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# 文内の単語のを取得するための関数を作成\n",
    "def find_xs_in_y(xs, y):\n",
    "    return [x for x in xs if y['begin'] <= x['begin'] and x['end'] <= y['end']]\n",
    "'''\n",
    "リスト内包表記\n",
    "for x in xs:\n",
    "    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\n",
    "        return x\n",
    "\n",
    "xs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\n",
    "'''\n",
    "\n",
    "#言語モデルの作成\n",
    "def create_language_model(doc_ids, N=3):\n",
    "    sents = []\n",
    "    \n",
    "    # コーパスとして文ごとに単語の原型のリストをsents変数に格納する\n",
    "    for doc_id in doc_ids:\n",
    "        all_tokens = datastore.get_annotation(doc_id, 'token')\n",
    "        \n",
    "        for sent in datastore.get_annotation(doc_id, 'sentence'):\n",
    "            tokens = find_xs_in_y(all_tokens, sent)\n",
    "            # コーパスとして文ごとに単語の原型リストをsents変数に格納する。文頭と文末も単語として追加する。\n",
    "            sents.append(['__BOS__'] + [token['lemma'] for token in tokens] + ['__EOS__'])\n",
    "            \n",
    "    # 作成したコーパスにおける全単語の一覧をvocabに作成\n",
    "    vocab = Vocabulary([word for sent in sents for word in sent])\n",
    "    # ngramsを使用し文ごとに3つの単語の並びを取り出す\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sents]\n",
    "    '''\n",
    "    ngramの第一引数には単語リスト、第2引数には関数で指定したデフォルトの3を入れる\n",
    "    '''\n",
    "    lm = MLE(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語らしさの確率を計算する関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (算出した言語モデル, 単語列, n-gramのn)\n",
    "def calc_prob(lm, lemmas, N=3):\n",
    "    probability = 1.0\n",
    "    \n",
    "    # ngramsを使用しN=3の単語の並びを作成し、一つずつ言語モデルを計算する。\n",
    "    for ngram in ngrams(lemmas, N):\n",
    "        prob = lm.score(lm.vocab.lookup(ngram[-1]), lm.vocab.lookup(ngram[:-1]))\n",
    "        # 学習したコーパスにない単語に関しては並びの確率に10の-8乗を割り当てる。\n",
    "        prob = max(prob, 1e-8)\n",
    "        #  計算した単語の並びの確率をかけ合わせて、文の確率を計算する。\n",
    "        probability *= prob\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算開始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ライブラリの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/vagrant/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/home/vagrant/.local/lib/python3.6/site-packages/IPython/extensions', '/home/vagrant/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# https://www.sejuku.net/blog/66459\n",
    "# システムに関する処理をまとめたライブラリのsysを読み込む\n",
    "import sys\n",
    "# 下記でライブラリを読み込めるパス一覧を表示できる。ここにパスを書き込むと異なる階層からライブラリを読み込む事が可能となる。\n",
    "print(sys.path)\n",
    "# sys.path.append(\"相対パス\")でsys.pathに追加、ここではディレクトリまでを指定する\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import sqlitedatastore as datastore\n",
    "import cabochaparser as parser\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.112896e-06: 古くから人が居住する\n",
      "3.392130e-19: 古くから居住する人が\n",
      "1.492537e-26: 古くから居住するが人\n",
      "7.462687e-27: 古くから人がする居住\n",
      "2.597403e-27: 古くからする人が居住\n",
      "4.782609e-33: 古くからが居住する人\n",
      "4.972376e-34: 古く人がから居住する\n",
      "4.972376e-34: 古くが人から居住する\n",
      "4.756185e-34: 古く人からが居住する\n",
      "2.486188e-34: 古くからが人居住する\n",
      "2.285714e-34: 古くするから人が居住\n",
      "2.186335e-34: 古く人が居住するから\n",
      "1.492537e-34: 古くから居住人するが\n",
      "1.492537e-34: 古くから居住人がする\n",
      "1.492537e-34: 古くから居住が人する\n",
      "1.492537e-34: 古くから居住がする人\n",
      "7.462687e-35: 古くから人居住するが\n",
      "7.462687e-35: 古くから人居住がする\n",
      "7.462687e-35: 古くから人する居住が\n",
      "7.462687e-35: 古くから人するが居住\n"
     ]
    }
   ],
   "source": [
    "datastore.connect()\n",
    "\n",
    "# 全てのidで言語モデルを作成\n",
    "lm = create_language_model(datastore.get_all_ids(limit=-1), N=3)\n",
    "\n",
    "# テストする文を設定\n",
    "text = '古くから人が居住する。'\n",
    "\n",
    "# cabochaを実行し、textをtokenに分割する\n",
    "sentences, chunks, tokens = parser.parse(text)\n",
    "\n",
    "probabilities = set([])\n",
    "for i in range(1000):\n",
    "    # 2語目以降のtokenをシャッフルしtokens_に収納\n",
    "    tokens_ = tokens[1:]\n",
    "    random.shuffle(tokens_)\n",
    "    # 1語目とくっつける\n",
    "    tokens_shuffled = [tokens[0]] + tokens_\n",
    "    lemmas = ['__BOS__'] + [token['lemma'] for token in tokens_shuffled] + ['__EOS__']\n",
    "    # 文ごとに改行を加える。\n",
    "    shuffled_text = ''.join([text[token['begin']:token['end']] for token in tokens_shuffled])\n",
    "    # 言語モデルを実行\n",
    "    probability = calc_prob(lm, lemmas, N=3)\n",
    "    # 結果をprobabilitiesに格納\n",
    "    probabilities.add((probability, shuffled_text))\n",
    "\n",
    "# probabilityの高い順に並び替え上位20件を取得する\n",
    "for probability, shuffled_text in sorted(list(probabilities), reverse=True)[:20]:\n",
    "    print(f'{probability:e}: {shuffled_text:s}')\n",
    "    \n",
    "datastore.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
