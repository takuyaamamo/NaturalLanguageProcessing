{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chainerのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo chmod 777 /usr/local/lib/python3.6/dist-packages/__pycache__/\n",
    "# !pip3 install chainer\n",
    "# !git clone https://github.com/chainer/chainer.git\n",
    "# !export PYTHONPATH=chainer/examples/text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "# 活性化関数等を管理する関数\n",
    "import chainer.function as F\n",
    "# BiasとWeightを管理する関数\n",
    "import chainer.links as L\n",
    "import numpy\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "import sys\n",
    "sys.path.append('/home/vagrant/chainer/examples/text_classification/')\n",
    "import nets\n",
    "import nlp_utils\n",
    "\n",
    "# 継承クラスの作成、class名はEncoder,別ファイルのchainer.Chainを継承する\n",
    "class Encoder(chainer.Chain):\n",
    "    \n",
    "    # def __init__はコンストラクタ\n",
    "    # 中には、どのように生成するか、どのようなデータを持たせるかなど、といった情報を定義する\n",
    "    def __init__(self, w):\n",
    "        # super(Encoder, self).__init__()で別ファイルのスーパークラス（chainer.Chain）のメソッドを呼び出すことが出来る。\n",
    "        super(Encoder, self).__init__()\n",
    "        # 300はWord2Vecの次元数\n",
    "        self.out_units = 300\n",
    "        \n",
    "        # with構文でファイルを扱う\n",
    "        # Chainクラスで重みの更新がされるのは self.init_scope()内に書いている linkオブジェクト\n",
    "        with self.init_scope():\n",
    "            self.embed = lambda x: F.embed_id(x, w)\n",
    "            # 学習するLSTMの形を設定する\n",
    "            self.encoder = L.NStepLSTM(n_layers=1, in_size=300, out_size=self.out_units, dropout=0.5)\n",
    "    \n",
    "    # 単語のID列をWord2Vecのベクトル列のデータに変換し、LSTMにわたす\n",
    "    def forward(self, ws):\n",
    "        exs = nets.sequence_embed(self.embed, xs)\n",
    "        last_h, last_c, ys = self.encoder(None, None, exs)\n",
    "        return last_h[-1]\n",
    "    \n",
    "# trainでモデルの学習を行う\n",
    "def train(labels, features, w):\n",
    "    # set型、集合型に変換する\n",
    "    n_class = len(set(labels))\n",
    "    print(f'# data: {len(features)}')\n",
    "    print(f'# class: {n_class}')\n",
    "    \n",
    "    # 学習用データをchainerのiteratorの形にしておく\n",
    "    pairs = [(vec, numpy.array([cls], numpy.int32)) for vec, cls in zip(features, labels)]\n",
    "    train_iter = chainer.iterators.Seriallterator(pairs, batch_size=16)\n",
    "    \n",
    "    # 学習するモデルをちゃいねｒのサンプルプログラムのTextClassifierクラスを用いて設定する\n",
    "    # 二値分類であるためカテゴリ数には2を指定、モデルはEncoderクラスのLSTMを指定する\n",
    "    model = nets.TextClassifier(Encoder(w), n_class)\n",
    "    \n",
    "    # 最適化にはAdamを選択\n",
    "    # ニューラルネットの学習方法を指定します。SGDは最も単純なものです。\n",
    "    optimizer = chainer.optimizers.Adam()\n",
    "    # 学習させたいパラメータを持ったChainをオプティマイザーにセットします。\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer.WeightDecay(1e-4))\n",
    "    \n",
    "    # optimizerを使用してupdatersでパラメータを更新する\n",
    "    updater = training.updaters.StandardUpdater(train_iter, optimizer, converter=convert_seq)\n",
    "    # Trainerを用意する。updaterを渡すことで使える。epochを指定する。outはデータを保存する場所。\n",
    "    trainer = training.Trainer(updater, (8, 'epoch'), out='./result/dl')\n",
    "    \n",
    "    # 下記で学習経過を確認する\n",
    "    trainer.extend(extensions.LogReport())\n",
    "    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'elapsed_time']))\n",
    "    \n",
    "    # 学習スタート\n",
    "    trainer.run()\n",
    "    return model\n",
    "\n",
    "# 分類の実行\n",
    "def classify(features, model):\n",
    "    with chainer.using_config('train', False), chainer.no_backprop_model():\n",
    "        prob = model.predict(features, softmax=True)\n",
    "    answers = model.xp.argmax(prob, axis=1)\n",
    "    return answers\n",
    "\n",
    "def convert_into_features_using_vocab(sentences, vocab):\n",
    "    contents = []\n",
    "    for doc_id, sent, tokens in sentences:\n",
    "        features = [token['lemma'] for token in tokens]\n",
    "        contents.append(features)\n",
    "    features = transform_to_array(contents, vocab, with_label=False)\n",
    "    return feautures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
