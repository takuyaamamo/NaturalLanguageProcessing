{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデルのN-gramとは\n",
    "\n",
    "- 言語モデル  \n",
    "単語の並びに対して、その発生確率を返すもの。通常はたんｇの出現頻度にもとづいて言語モデルが算出される。\n",
    "- N-gramモデル  \n",
    "言語モデルの一種で「単語の発生確率がその前のN-1個の単語にのみ依存する」と仮定したモデル。  \n",
    "（機械学習等に使うテキストの特徴量のことをN-gramという事もある）\n",
    "\n",
    "例）  \n",
    "N=3のときの、3-gramモデルは、単語の出現確率が、その前の2（=3-1）個の単語にのみ依存すると仮定したモデル。  \n",
    "- 3-gramを求める  \n",
    "$$\n",
    "P(\"[BOS]\"　\"古く\"　\"から\"　\"人\"　\"が\"　\"居住\"　\"する\"　\"[EOS]\") = \\\\\n",
    "P(\"[BOS]\",\"古く\") \\times P(\"から\" | \"[BOS]\",\"古く\") \\times P(\"人\" | \"古く\",\"から\") \\times P(\"が\" | \"から\",\"人\") \\times P(\"居住\" |  \"人\",\"が\") \\times P(\"する\" | \"が\",\"居住\") \\times P(\"[EOS]\" | \"居住\",\"する\")\n",
    "$$\n",
    "\n",
    "単語「人」が出現する確率に注目すると\n",
    "\n",
    "$$\n",
    "P(\"人\" | \"古く\",\"から\")= \\\\\n",
    "\\displaystyle \\frac{ コーパスの中で単語の「古く」「から」が、この順で現れたあとに「人」が現れる回数 }{ コーパスの中で単語の「古く」「から」が、この順で現れる回数 } \n",
    "$$\n",
    "\n",
    "として計算する事ができます。  \n",
    "同様に全ての単語に対して3-gramを計算しておくと\n",
    "$$\n",
    "P(\"[BOS]\" \"古く\" \"から\" \"人\" \"が\"　\"居住\"　\"する\"　\"[EOS]\") \\\\\n",
    "P(\"[BOS]\" \"古く\" \"が\" \"人\" \"から\"　\"居住\"　\"する\"　\"[EOS]\")\n",
    "$$\n",
    "のどちらが自然な文かを比較できる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramを計算してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltkライブラリをインストールする  \n",
    "Pythonで自然言語処理を行うために便利な機能を持つライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "Collecting six (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, nltk\n",
      "Successfully installed nltk-3.4.5 six-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqlite3接続用、今回はパスを通してスクリプトとして読み込んで見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/vagrant/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/home/vagrant/.local/lib/python3.6/site-packages/IPython/extensions', '/home/vagrant/.ipython', 'src/sqlitedatastore.py', 'src', 'src']\n"
     ]
    }
   ],
   "source": [
    "# システムに関する処理をまとめたライブラリのsysを読み込む\n",
    "import sys\n",
    "# 下記でライブラリを読み込めるパス一覧を表示できる。ここにパスを書き込むと異なる階層からライブラリを読み込む事が可能となる。\n",
    "print(sys.path)\n",
    "# sys.path.append(\"相対パス\")でsys.pathに追加、ここではディレクトリまでを指定する\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import sqlitedatastore as datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nリスト内包表記\\nfor x in xs:\\n    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\\n        return x\\n\\nxs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文内の単語のを取得するための関数を作成\n",
    "def find_xs_in_y(xs, y):\n",
    "    return [x for x in xs if y['begin'] <= x['begin'] and x['end'] <= y['end']]\n",
    "'''\n",
    "リスト内包表記\n",
    "for x in xs:\n",
    "    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\n",
    "        return x\n",
    "\n",
    "xs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#言語モデルの作成\n",
    "def create_laguage_model(doc_ids, N=3):\n",
    "    sents = []\n",
    "    \n",
    "    # コーパスとして文ごとに単語の原型のリストをsents変数に格納する\n",
    "    for doc_id in doc_ids:\n",
    "        all_tokens = get_annotation(doc_id, 'token')\n",
    "        \n",
    "        for sent in datastore.get_annotation(doc_id, 'sentence'):\n",
    "            tokens = find_xs_in_y(all_tokens, sent)\n",
    "            sents.append(['__BOS__'] + [token['lemma']\n",
    "                                        for token in tokens] + ['__EOS__'])\n",
    "    vocab = Vocabulary([word for sent in sents for word in sent])\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sents]\n",
    "    lm = MLE(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
