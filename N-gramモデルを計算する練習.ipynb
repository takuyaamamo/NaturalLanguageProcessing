{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデルのN-gramとは\n",
    "\n",
    "- 言語モデル  \n",
    "単語の並びに対して、その発生確率を返すもの。通常はたんｇの出現頻度にもとづいて言語モデルが算出される。\n",
    "- N-gramモデル  \n",
    "言語モデルの一種で「単語の発生確率がその前のN-1個の単語にのみ依存する」と仮定したモデル。  \n",
    "（機械学習等に使うテキストの特徴量のことをN-gramという事もある）\n",
    "\n",
    "例）  \n",
    "N=3のときの、3-gramモデルは、単語の出現確率が、その前の2（=3-1）個の単語にのみ依存すると仮定したモデル。  \n",
    "- 3-gramを求める  \n",
    "\n",
    "$$\n",
    "P(\"[BOS]\"　\"古く\"　\"から\"　\"人\"　\"が\"　\"居住\"　\"する\"　\"[EOS]\") = \\\\\n",
    "P(\"[BOS]\",\"古く\") \\times P(\"から\" | \"[BOS]\",\"古く\") \\times P(\"人\" | \"古く\",\"から\") \\times P(\"が\" | \"から\",\"人\") \\times P(\"居住\" |  \"人\",\"が\") \\times P(\"する\" | \"が\",\"居住\") \\times P(\"[EOS]\" | \"居住\",\"する\")\n",
    "$$\n",
    "\n",
    "単語「人」が出現する確率に注目すると\n",
    "\n",
    "$$\n",
    "P(\"人\" | \"古く\",\"から\")= \\\\\n",
    "\\displaystyle \\frac{ コーパスの中で単語の「古く」「から」が、この順で現れたあとに「人」が現れる回数 }{ コーパスの中で単語の「古く」「から」が、この順で現れる回数 } \n",
    "$$\n",
    "\n",
    "として計算する事ができます。  \n",
    "同様に全ての単語に対して3-gramを計算しておくと\n",
    "\n",
    "$$\n",
    "P(\"[BOS]\" \"古く\" \"から\" \"人\" \"が\"　\"居住\"　\"する\"　\"[EOS]\") \\\\\n",
    "P(\"[BOS]\" \"古く\" \"が\" \"人\" \"から\"　\"居住\"　\"する\"　\"[EOS]\")\n",
    "$$\n",
    "\n",
    "のどちらが自然な文かを比較できる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramを計算してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nltkライブラリをインストールする  \n",
    "Pythonで自然言語処理を行うために便利な機能を持つライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "Collecting six (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, nltk\n",
      "Successfully installed nltk-3.4.5 six-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sqlite3接続用、今回はパスを通してスクリプトとして読み込んで見る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/vagrant/.local/lib/python3.6/site-packages', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/home/vagrant/.local/lib/python3.6/site-packages/IPython/extensions', '/home/vagrant/.ipython']\n"
     ]
    }
   ],
   "source": [
    "# https://www.sejuku.net/blog/66459\n",
    "# システムに関する処理をまとめたライブラリのsysを読み込む\n",
    "import sys\n",
    "# 下記でライブラリを読み込めるパス一覧を表示できる。ここにパスを書き込むと異なる階層からライブラリを読み込む事が可能となる。\n",
    "print(sys.path)\n",
    "# sys.path.append(\"相対パス\")でsys.pathに追加、ここではディレクトリまでを指定する\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "import sqlitedatastore as datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nリスト内包表記\\nfor x in xs:\\n    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\\n        return x\\n\\nxs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文内の単語のを取得するための関数を作成\n",
    "def find_xs_in_y(xs, y):\n",
    "    return [x for x in xs if y['begin'] <= x['begin'] and x['end'] <= y['end']]\n",
    "'''\n",
    "リスト内包表記\n",
    "for x in xs:\n",
    "    if y['begin'] <= x['begin'] and x['end'] <= y['end']:\n",
    "        return x\n",
    "\n",
    "xs変数に格納されているアノテーションのリストからyアノテーションの内側に存在するものだけを取り出す関数\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 言語モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "# MLE(Maximum Likelihood Estimator):言語モデルを作成するライブラリ\n",
    "from nltk.util import ngrams\n",
    "\n",
    "#言語モデルの作成\n",
    "def create_language_model(doc_ids, N=3):\n",
    "    sents = []\n",
    "    \n",
    "    # コーパスとして文ごとに単語の原型のリストをsents変数に格納する\n",
    "    for doc_id in doc_ids:\n",
    "        all_tokens = datastore.get_annotation(doc_id, 'token')\n",
    "        \n",
    "        for sent in datastore.get_annotation(doc_id, 'sentence'):\n",
    "            tokens = find_xs_in_y(all_tokens, sent)\n",
    "            # コーパスとして文ごとに単語の原型リストをsents変数に格納する。文頭と文末も単語として追加する。\n",
    "            sents.append(['__BOS__'] + [token['lemma'] for token in tokens] + ['__EOS__'])\n",
    "            \n",
    "    # 作成したコーパスにおける全単語の一覧をvocabに作成\n",
    "    vocab = Vocabulary([word for sent in sents for word in sent])\n",
    "    # ngramsを使用し文ごとに3つの単語の並びを取り出す\n",
    "    text_ngrams = [ngrams(sent, N) for sent in sents]\n",
    "    '''\n",
    "    ngramの第一引数には単語リスト、第2引数には関数で指定したデフォルトの3を入れる\n",
    "    '''\n",
    "    lm = MLE(order=N, vocabulary=vocab)\n",
    "    lm.fit(text_ngrams)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 20775 items>\n",
      "('古く', 'から') ->\n",
      "\tの: 0.089552\n",
      "\tこの: 0.044776\n",
      "\t行う: 0.044776\n",
      "\t盛ん: 0.044776\n",
      "\t航空: 0.029851\n",
      "\t音楽: 0.029851\n",
      "\t居住: 0.029851\n",
      "\t広い: 0.014925\n",
      "\t活発: 0.014925\n",
      "\t関係: 0.014925\n",
      "\t海運: 0.014925\n",
      "\t海上: 0.014925\n",
      "\t整える: 0.014925\n",
      "\t文明: 0.014925\n",
      "\tアフガニスタン: 0.014925\n",
      "\t*: 0.014925\n",
      "\t建設: 0.014925\n",
      "\tフォアグラ: 0.014925\n",
      "\t伝わる: 0.014925\n",
      "\t知る: 0.014925\n",
      "\t多様: 0.014925\n",
      "\t中国: 0.014925\n",
      "\t交流: 0.014925\n",
      "\t造成: 0.014925\n",
      "\t北: 0.014925\n",
      "\tワイン: 0.014925\n",
      "\t使う: 0.014925\n",
      "\t息衝く: 0.014925\n",
      "\tアイスランド: 0.014925\n",
      "\t定住: 0.014925\n",
      "\t民間: 0.014925\n",
      "\t地中: 0.014925\n",
      "\t言語: 0.014925\n",
      "\t下: 0.014925\n",
      "\tモン: 0.014925\n",
      "\t軍備: 0.014925\n",
      "\t水上: 0.014925\n",
      "\t世界: 0.014925\n",
      "\t紛争: 0.014925\n",
      "\t無数: 0.014925\n",
      "\t農業: 0.014925\n",
      "\t利用: 0.014925\n",
      "\t繁栄: 0.014925\n",
      "\t印欧語: 0.014925\n",
      "\t国: 0.014925\n",
      "\t高い: 0.014925\n",
      "\t人: 0.014925\n",
      "\tバスケットボール: 0.014925\n",
      "\t発達: 0.014925\n",
      "\t住民: 0.014925\n",
      "\t自動車: 0.014925\n",
      "\t中東: 0.014925\n",
      "\t通商: 0.014925\n"
     ]
    }
   ],
   "source": [
    "datastore.connect()\n",
    "\n",
    "# 言語モデルを実行\n",
    "lm = create_language_model(datastore.get_all_ids(limit=-1), N=3)\n",
    "\n",
    "# 「古く」「から」という2単語の並びに続く単語の出現確率を言語モデルから呼び出す。\n",
    "context = ('古く', 'から')\n",
    "print(context, '->')\n",
    "\n",
    "# 出現確率を言語モデルから呼び出す\n",
    "prob_list = [(word, lm.score(word, context)) for word in lm.context_counts(lm.vocab.lookup(context))]\n",
    "# 確率の大きい順にソート\n",
    "prob_list.sort(key=lambda x: x[1], reverse=True)\n",
    "for word, prob in prob_list:\n",
    "    print('\\t{:s}: {:f}'.format(word, prob))\n",
    "datastore.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
